# 

https://wikidocs.net/book/2350 scala참고!

## Scala

스칼라는 2004년 마틴 오더스키(Martin Odersky)가 발표한 **객체 지향 언어의 특징과 함수형 언어의 특징을 함께 가지는 다중 패러다임 프로그래밍 언어**다.

***

## 특징

1. 객체지향 언어의 특성 +함수형 언어의 특성(자바스크립트랑 파이썬, R도 비슷)
2. 자바가상머신(JVM)에서 동작하는 JVML언어
   - 자바가상머신위에서 동작하는 것은 scala,kotlin,groovy등
3. 자바 가산머신위에서 동작하기에 자바의 모든 라이브러리 사용 가능 
4. 스칼라 컴파일러를 통해 스칼라 코드로 변환하고, 바이트 코드는 JVM상에서 자바와 동일하게 실행(즉 플랫폼에 독립적이다, Windows나 Linux나 Mac에  동일하게 실행)

### 함수형 언어

1. 함수형 언어 특성으로 자바에 비해 코드 길이가 짧다.

   - **적은 양의 코드로 방대한 규모의 시스템을 작성할 수 있다.**
   - 스칼라에서는 모든것이 Object이기 때문에 ==로 모든 비교 가능

2. 객체지향 프로그래밍 언어와 함수형 프로그래밍 모두 완벽하게 지원

   - 모든 것이 객체이면 함수가 first object(함수적 프로그래밍 특성은 함수를 마치 하나의 값으로 취급하여 변수 또는 파라미터로 넘길 수 있다.)
   - 모든 것을 함수로 해결하면 의도하지 않는 동작이 발생 할 일이 없고, 한번 검증된 함수는 신뢰할 수 있기 때문에 버그가 줄어든다.
   - Immutable 변수는 문제를 단수화 시켜주어 데이터 공유, 병렬처리에 강하다.

   #### 함수 언어

   > 함수의 실행이 외부에 영향을 끼치지 않는 함수. 

   #### 익명 함수

   > 선언부가 없는 함수. 코드의 길이를 줄일 수 있다.
   >
   > Arrays.asList(1,2,3).stream().reduce((a,b) -> a-b).get();
   >
   > 여기서 reduce가 익명함수이다.

   #### 고차 함수

   > 함수를 인수로 취하는 함수. 함수를 입력 파라미터나 출력 값으로 처리 가능
   >
   > 

### 장점

1. 코드의 직관성, 신축성
2. 풍부한 표현식과 연산자
   - Fist-class function
   - Closure
3. 간결함
   - 타입 추론
   - 함수 리터럴(Literal)
4. 자바와의 혼용가능 객체지향+함수형 언어
   - 자바 라이브러리 재사용 가능
   - 자바 도구를 재사요 ㅇ가능
   - 성능의 손실 없이 사용 가능
   - 스칼라에서는 모든 것이 객체
5. 동시성에 강한 언어
   - 스칼라에서는 많은부분이 변경불가능 속성을 가지게끔 되어있다.
   - 아카(AKKa)라이브러리=동시성 프로그래밍에서 뛰어난 액터(=**함수**) 모델이용
   - 액터 모델은 각각의 액터가 서로 간의 메시지를 통해서만 의사소통을 하고  액터를 이루는 각종 변수나 속성은 서로 공유하지 않는다.
6. expression( 결과를 반환하는 문장) 중심 언어=표현중심언어
7. 필요할 때 implicit 예약어를 사용하면 명시적인 표현을 감출 수있다.

## Scala설치

1. 윈도우에서 해도 된다.
2. https://www.scala-lang.org/download/ (혹은 구글에서 스칼라 다운로드 검색) [scala-2.13.0.msi](https://downloads.lightbend.com/scala/2.13.0/scala-2.13.0.msi) 다운
3. Scala IDE-IntelliJ, or sbt,Scala's build tool. Http://scala-ide.org 
4. 이클립스는 마켓플레이스에서 스칼라 IDE를 설치하면 슼라라 프로젝트 생성 가능 scala ide 검색

#### 실행

1. cmd 창을 열고
2. scala 입력
3. prompt 가 열린다. (스칼라 셀은 스칼라 )
   - 안되는 경우 환경변수 추가 하자 
   - Path에서 `C:\Program Files (x86)\scala\bin` 추가

#### 기초 

```cmd
scala>1
scala>var a=1
scala>val b=1
scala>b=2 #하면 오류생긴다(val은 불변 변수 선언이므로)
scala>val c:Int=0 #변수 타입 지정
scala>1 to 10
sclaa>(1 to 10).toList
scala>1.toDouble #형변환
scala>1.0.toInt #형변환
```

- 스칼라의 기본 변수 타입은 모두 클래스
- 변수 선언 var,val(불변 변수 선언),변수 타입 생략 가능
- Range 타입 -1 to 10, 1 to 10 by 2 , 리스트나 배열 타입으로는 형변환 해야 한다.

```cmd
scala>:help
:edit
:help
:history
:load <path>
:quit
:save <path>
```

```scala
object Ex1{
def main(args: Array[String]): Unit={
println("Hello, Scala!")
}
}
```

메모장에 제목 "Ex1.scala" 로 저장

실행은 cmd 창에서 저장된 위치로 가서 

```cmd
C:\scala> scalac Ex1.scala # C폴더 아래 scala라는 파일을 생성한 상태, scalac 로 컴파일 
C:\scala> dir/w #파일 확인
C:\scala>scala Ex1
Hello, Scala!
#실행확인

```



- null보다는 none을 추천

- var는 언제든지 값이 바뀔 수 있는 일반적인 변수 선언

- val는 final변수 선언

- 변수 의 값으로 null이나 None으로 초기화 가능

- 컴파일러가 알아서 자료형에 대해 판단하고 필요하면 묵시적 형 변환을 통해 필요한 자료형으로 바꿔준다.

- 자료형을 명시적으로 선언하여 해당 자료형으로 값이 저장

- 기본 자료형은 자바에서 파생문자열인 String을 제외하고 AnyVal이라는 공통의 이름으로 불리며, 참조 자료형은 AnyRef로 불린다.

  ```cmd
  scala>var d:Int=0.1
  scala>var e=null
  scala>d=None
  scala>var f:Boolean=1 #에러가 남으로 호환이 안된다.
  
  scala>val str2="""a
  b
  c"""
  #멀티 라인 문자열은 세개의 쌍따옴표를 이용
  ```



#### 접두어

- 접두어를 이용한 처리

- 접두어 S

- ${변수명}을 이용하여 문자열 안의 변수를 값으로 치환

  ```cmd
  scala> val name="David"
  scala>println(s"Hello! ${name}") 
  sclala>println("${1+1}")
  scala>println(s"${1+1}")
  ```

- 접두어 F

- 문자열 포맷팅 처리 , 자바의 pirnf()와 같은 방식

  ```scala
  val height:Double=182.3
  val name="james"
  //f접두어를 이용한 값 변화 테스트
  println(f"$name%s is $height%2.2f meters tall")
  ```

- 접두어 raw

- 접수어 raw는 특수 문자를 처리하지 않고 원본 문자로 인식

  ```scala
  s"가\n나" //\n으로 개행 문자 처리(엔터)
  raw"가\n나"//\n을 문자 그대로 인식
  ```

##### 접두어 예제

```scala
var str3=s"println $str1"
println(str3)
println(s"2*3=${2*3}")
def minus(x: Int, y:Int)=x-y
println(s"${Math.pow(2,3)}") //2를 3번 곱한것 (지수승)
println(s"${minus(2,3)}")
```

#### Range타입

1. Range 타입 -1 to 10, 1 to 10 by 2, 리스트나 배열 타입으로는 형변환 해야한다

2. type 예약어는 자료형이 복잡한 경우 변칭을 주어 쓸 수 있게 한다.

   ```scala
   type Name=String
   type Person=(String,Int)
   type Ftype=String=>Int //함수 표현식
   val name: Name="홍길동"
   val person:Person=("korea",24)
   val f:Ftype=text=>text.toInt
   println(f) //객체 생성 됐따는 것
   f("34.125")//에러
   f(34.125)//에러 
   f(34) //에러
   f("34")//Int라 뜬다
   ```

#### 조건문, 반복문

- 조건문 : if /else
- 반복문 :for, while, do while

```scala
for(x<-1 to 10){
    //반복할 실행문
    println(x)// 10까지 나온다.
}
for(x<-1 range){
    //반복할 실행문
    println(x)
}
for(x<-1 until 10){
    //반복한 실행문
    println(x) //9까지 나온다.
}
//조건이 있는 반복문
for(i <-10) if (i % 2==0){
    println(i)
}
for(i <- 1 to 10) if (i % 2==0){
    println(i)
}
val lst=(10 to 100 by 10).toList
//인덱스가 있는 for ans
for((num,index) <- lst.zipWithIndex){
    println(s"index: $num")
}
```

- 이중 for 문

```scala
object Ex{
    def main(args: Array[String]): Unit={
        for(x <- 1 until 5; y <- 1 until 5){
            print(x + "*" + y + "=" + x*y +"|")
        }
    }
} //저장해서 scalac 하거나 for문만 cmd에서 scala에 실행
```



#### 함수

- 변수와 마찬가지로 :를 이용해 반환 자료형을 정의 , 반환 자료형이 함수의 자료형을 결정
- Unit은 자바의 void 자료형과 같다(반환 결과가 없는 함수에 붙는 자료형)
- 반환 값이 있을 때도 반환 자료형 생략 가능
- 명시적으로 return을 사용하는 경우, 함수 선언하는 곳에도 반환 자료형을 명시
- 반환 자료형을 명시한 경우 다른 자료형을 반환하려고 하면 에러 발생

```scala
def 함수([매개변수]): [반환 자료형]= {
//구현할 로직
}
```

```scala
object Ex{
def main(args: Array[String]): Unit = {
println("반환받은 값: "+ name())
}
def name()={
val a=10
a
}
}
```

```scala
def name() :  Int={
val a=10
return a
}
```

```scala
def name() = {
val a=10
return a
} //error :method name has return statement; needs result type
```

```scala
def name3(): Int={
val a=10
a
}
```

- 스칼라에서는 함수 인자의 타입을 명시해야한다

- 인자가 없는 함수의 경우 호출시 괄호 생략 가능

- ```scala
  def addOne(m: Int):Int =m+1
  val three =addOne(2)
  def three()=1+2
  three()
  three
  ```

- **이름없는 함수**를 만들 수 있다.

- 이름 없는 함수르 ㄹ다른 함수나 식에 넘기거나 val에 저장 가능

- 함수가 여러 식으로 이루어진 경우 {}를 사용해 이를 위한 공간 만들 수 있다.

- ```scala
  (x:Int) => x+1
  인터프리터가 부여한 이름(1)
  
  val addOne=(x:Int)=> x+1
  addOne(1)
  ```

- ```scala
  def timesTwo(i:Int):Int={
  println("hello world")
  i*2
  }
  timesTwo(2)
  
  {i:Int=>
  println("hello world")
  i*2} //즉시 실행 함수, 익명함수
  ```

- **(_) 사용하기**

- 함수 호출시 (_)사용하면 일부만 적용 가능, 그렇게 하면 새로운 함수 얻는다. 스칼라에서 밑줄은 문맥에 따라 의미가 다르다.

- 보통 이름없는 마법의 문자!

- 인자 중에서 원하는 어떤 것이든 부분 적용 가능, 위치는 아무곳이나 가능

- ```scala
  def adder(m: Int, n:Int) =m+n
  val add2=adder(2,_:Int)
  add2(3)
  ```

- **커리함수**

- 함수의 인자중 일부 적용하고, 나머지는 나중에 적용하게 남겨두는 것

- ```scala
  object Ex{
      def main(args: Array[String]): Unit={
          val thisYear = 2009
          val fixedValueFunction=go(thisYear, _:String)
          fixedValueFunction("test1")
          fixedValueFunction("test2")
          fixedValueFunction("test3")
      }
      def go(thisYear:Int, String:String)={
          println(String + ":" + thisYear)
      }
  }
  ```

- **=>**를 이용한 함수 표현식

- 스칼라 컴파일러는 = >표현식을 보면 function객체로 선언

- 스칼라는 매개변수가 하나면 function1, 두개면 function2,...22개 까지 function을 상속 하는 트레이트 제공

- ```scala
  def functionAsValue=(y:Int)=> y+10
  //컴파일러가 아래로 컴파일 한다
  val functionAsValue : Int=>Int=new Function(Int,Int){
      def apply(y:Int):Int=y+10
  }//컴파일러가 이리 해주기에 우리는 위의 한줄로만 써도 된다.
  ```

- **변수가 함수 넣기**

- 명시적으로 함수가 기대되지 않는 곳에서 =연산자를 이용해 매개변수가 필요한 함수를 대입하였을때 에러가 발생-객체화되어 있찌 않는 함수를 바로  val에 대입하면 에러 발생

- ```scala
  Object Ex{
      def main(args: Array[String]): Unit={
          val g=f_
          println(f(1))
      }
      def f(i:Int)=i
  }//에러발생?
  ```

- ```scala
  val g(Int=>Int)=f//이리 써야 한다?
  ```

- ```scala
  val g=f //는 그대로 두고
  //f 를 선언할 때 =>를 이용
  def f=(i:Int)=>i
  ```

- **재귀함수**

- ```scala
  object Ex{
      def main(args: Array[String]): Unit={
          val result=calc(x => x*x, 2,5)
          println(result)
      }
      def calc(f:Int = > Int, start: Int, end: Int) = {
          //합계를 구하는 재귀 함수
          def loop(index:Int, sum:Int):Int={
              if(index>end) sum//인덱스가 끝 값보다 크면 sum을 반환
              else loop(index+1, f(index)+sum)
          }
          loop(start,0)//루프를 최초 호출
      }
  }
  ```

- **매개변수가 여러 개인 함수**

  ```scala
  object Ex{
      def printlnStrings(args: String*)={
          for(arg <- args){
              println(arg);
          }
      }
  }
  
  printlnStrings("st1","st2","st3")
  printlnStrings( )
  
  val array1=Array("1","2","3","4")
  printlnStrings(array1) //String을 요구하는데 Array[String]가 들어가 오류 뜬다. 
  ```

-  **매개변수의 기본값 설정**

  ```scala
  object Ex{
      def default(a: Int=4,b:Int=5):Int=a+b
      println("기본값은"+default())
      println("변수값은"+default(11,6))
  }
  ```

- **apply**

- 매번 매서드 이름을 적는 것을 피하기 위해 사용

- 변수를 받아 함수에 적용시켜 결과를 만들어내는 설정자와 같은 역활

- Apply() 를 이용하면 생성자처럼 초기화하거나 클래스 안에 특정한 메서드를 기본 메서드로 지정하는 것을 편하게 할 수 있따.

  ```scala
  object Ex{
      class SomeClass{
          def apply(m:Int)=method(m)
          def method(i:Int)={
              println("method(Int) called")
              i+i
          }
          def method2(s:String)=5
      }
      val something=new SomeClass
      println(somethin(2))
  }
  ```

- **암묵적 형변환**

- Implicit 는 에러는 바로 내지 않고 해당하는 함수가 있으면 그것을 사용해서 암묵적으로 형 변환을 도와주어 함수의 활용도를 높힌다.

  ```scala
  object Sample{
  case class Person(name:String)
  //implicit def StringToPerson(name:String) :Person=Person(name) 이게 없으면 sayHello가 형변환이 안되고 이 게 있으면 자동 형변환된다
  def sayHello(p:Person): Unit ={
  print("Hello,"+p.name)
  }
  sayHello("korea")
  }
  ```

- 반환 자료형과 매개변수만으로 판단하기 때문에 에러 발생 할 수 있다.

  ```scala
  implicit def doubleToInt(d: Double)=d.toInt
  implicit def doubleToInt2(d: Double)=d.toInt+1
  val x:Int =18.0//에러 발생
  
  def doubleToInt(d:Double)=d.toInt
  val x:Int=doubleToInt(18.0)
  
  def doubleToInt(d:Double)=d.toInt
  val x:Int=18.0//에러 발생
  
  implicit def doubleToInt(d:Double)=d.toInt
  val x:Int =18.0
  ```

- **객체**

- 스칼라에서는 연산자와 메서드를 포함한 모든  것이 객체

- 객체 생성 방법

  - 클래스를 통한 인스턴스 화 -new 를 사용하여 계속 인스턴스 생성 가능
  - object예약어를 통해 객체 생성-싱글턴 객체로서 유일한 객체 생성

- 클래스

  - 클래스 안에서 메소드는 def로, 필드는 val로 정의한다. 메소드는 단지 클래스(객체)의 상태를 접근할 수 있는 함수에 지나지 않는다

- 스칼라에서는 public class대신 object예약어를 통해 처음부터 메모리에 객체 생성하고 컴파일러는 객체에 main이라는 이름이 있으면 main을 프로그램의 시작점으로 컴파일 한다.

  ```scala
  object Ex{
      def main(args: Array[String]): Unit={
          val apple=new Fruit("사과")
          println(apple.name)
      }
  }
  clss Fruit(input:String){
      var name=input
  }
  ```

- **케이스 클래스**

- 자동으로 멤버 변수를 만들어 주고, 외부에서도 멤버 변수에 접근이 가능하도록 한다. toString,hashCode,equals를 자동으로 만들어 준다.

  ```scala
  case class Fruit2(name:String)
  val apple=new Fruit2("사과")
  println(apple.name)
  val apple2=new Fruit2("사과")
  println(apple2.name)
  
  println(apple2.equals(apple)) //true
  println(apple.hashCode)
  println(apple.toString)//Fruit2(사과) 로 결과가 나온다.
  ```

- 스칼라에서는 생성자가 특별한 메소드를 따로 존재하지 않는다. 클래스 몸체에서 메소드 정의 부분밖에 있는 모든 코드가 생성자 코드가 된다.

  ```scala
  class Calculator(brand:String){
      //생성자
      val color:String=if(brand=="TI"){
          "blue"
      }else if(brand=="HP"){
      "black"
  }else{
      "white"
  }
  
  //인스턴스 메소드
  def add(m:Int, n:Int):Int=m+n
  }
  val calc=new Calculator("HP")
  calc.color
  ```

- 상속과 메소드 Overloading

  ```scala
  //상속
  class ScientificCalculator(brand:String) extends Calculator(brand){
      def log(m: Double,base:Double)math.log(m) / math.log(base)
  }
  
  ```

- **추상클래스 ** 는 메소드 정의는 있지만 구현은 없는 클래스이다. 대신 이를 상속한 하위클래스에서 메소드를 구현하게 된다. 추상 클래스의 인스턴스를 만들 수는 없다.

```scala
abstract class Shape{
    def getArea():Int
}
class Circle(r:Int)extends Shape{
    def getArea():Int ={r*r*3}
}
val s=new Shape //abstract이므로 변경 불가능
val c=new Circle(2) //
```

- **트레잇(trait)**
- 특성 : 하나의 완성된 기능이 아닌 어떤 한 객체에 추가될 수 있는 부가적인 특성
- 클래스의 부가적인 특성으로 동작, 자체로 인스턴스화는 가능하지 않다.
- 다른 클래스가 확장(즉, 상속) 하거나 섞어 넣을 수 있는(이를 믹스인 Mix in 이라 한다.)필드와 동작의 모음
- 클래스는 여러 트레잇을 with키워드를 사용해 확장
- 자유롭게 변수 선언하고 로직을 구현

```scala
trait Car{
    val brand: String
    
}
trait Shiny{
    val shineRefraction:Int
}
/*class BMW extends Car{
    val brand="BMW"
}*/
calss BMW extends Car with Shiny{//with로 확장
    val brand="BMW"
    val shineRefracton=12
}

//혹은
case class Car(brand:String)
case class Shiny(shineRefraction:Int)

```

- 추상클래스 대신 트레잇을 사용해야 하는 경우
- 인터페이스 역확을 하는 타입을  설계할때 트레잇과 추상클래스 두 가지 다 어떤 동작을 하는 타입을 만들수 있으며, 확장하는 쪽에서 일부를 구현하도록 요청
- 클래스는 오직 하나 만 상속 가능, 트레잇은 여러가지 상속 가능
- **트레잇 쌓기**
- 여러개를 한 클래스에서 상속받는데 상속에 문제가 생길경우(메소드를 호출했는데 다양한 클래스에 같은 메소드로 인한 충돌 발생) override 예약어와 함꼐 적당한 상속 관계를 만들 수 있다.
- 최종적인 상속받은 클래스의 메서드가 수행되도록 한다.

```scala
abstract class Robot{
    def shoot="뿅뿅"
}
trait M16 extends Robot{
    override def shoot="빵야"
}
trait Bazooka extends Robot{
    override def shoot="뿌왕뿌왕"
}
trait Daepodong extends Robot{
    override def shoot="콰르르르르를ㅇ"
}
class Mazinga extends Robot with M16 with Bazooka with Daepodong 
val robot=new Mazinga
println(robot.shoot)
//"콰르르르르르를ㅇ"이 출력된다.
```

- 모두의 기능을 실행하도록 상위클래스 super를 호출해서 해당하는 메서드를 실행되게 할 수 있다.

```scala
abstract class AnotherRobot{
    def shoot="뿅뿅"
}
trait M16 extends AnotherRobot{
    override def shoot=super.shoot + "빵야"
}
trait Bazooka extends AnotherRobot{
    override def shoot=super.shoot + "뿌왕뿌왕"
}
trait Daepodong extends AnotherRobot{
    override def shoot=super.shoot + "콰르르르르를ㅇ"
}
class Mazinga extends AnotherRobot with M16 with Bazooka with Daepodong 
val robot=new Mazinga
println(robot.shoot) //하면 모든것이 합쳐져서 나온다.
```

- **Compaion Object** static이 필요한 경우 대신 사용 가능
- **패턴 매치**
- 기본형이 아닌 튜플을 사용하는 경우 튜플 형식으로 변수를 정의해야 매칭된다.
- 리스트의 경우도 각 위치에 해당하는 값이 변수로 할당된다.
- 케이스 클래스의 경우는 클래스 형태를 그대로 사용해서 속성 정보를 매칭 할 수 있다.

```scala
val b=1
b match{
    case v if v==1 => println("b")
    case _ => println("err")
}
val c=(a,b)
c match{
    case(c1,c2) => println("c1:" + c1)
}
val d=List(1,3,5)
d match{
    case e1::e2::xs => 
}
```

```scala
def matchFunction(input:Any): Any =input match{
    case 100 => "hundred"
    case "hundred" => 100
    case etcNumber: Int => "입력값은 100이 아닌 Int형 정수입니다."
    case _ => "기타"
}
println(matchFunction(100))
println(matchFunction("hundred"))
println(matchFunction(1000))
println(matchFunction(1000.5))
println(matchFunction("thousand"))
```

```scala
case class Person(name:String, age:Int)
val alice=new Person("Alice",25)
val bob= new Person("Bob",32)
val charlie=new Person("Charlie",32)

for(person <- List(alice,bob,charlie)){
    person match{
        case Person("Alice",25) => println("Hi Alice!")
        case Person("Bob",32) => println("Hi Bob!")
        case Person(name,age)=> println(
        "Age: "+ age + " year, name: " + name + "?")
    }
}
```

- **Extractor로 패턴 매칭**
- Extractor는 패턴 매칭을 해야 하는 대상 값을 가져와서 필요한 값을 추출한 후 가공해서 내보낼 수 있다. 

```scala
object Emergency{
    def unapply(number: String):Boolean={
        if (number.length==3&&number.forall(_.isDigit)) true 
        else false
    }
}
object Normal{
    def unapply(number: String): Option[Int]={
        try{
            var o=number.replaceAll("-","")
            Some(number.replaceAll("-","").toInt)
        }catch{
            case _: Throwable => None
        }
    }
}
var number1="010-222-2222"
var number2="119"
var number3="포도 먹은 돼지"
var numberList=List(number1,number2,number3)
for(number <- numberList){
    number match{
        case Emergency() => println("긴급전화입니다.")
        case Normal(number) => println("일반전화입니다 - " + number)
        case _ => println("판단 할 수 없습니다.")
    }
}
```

- **컬렉션**
- ***배열*** 초기 값을 지정해서 배열을 선언하는 경우 자료형을 표시하지 않아도 알아서 자료형 할당

- ***리스트***  앞뒤가 연결된 리스트로서 내부적으로 리스트를 붙이거나 나누는데 효육적인 구조를 가지고 있다
- 동적으로 크기를 늘리거나 줄이는 것이 가능
- LIst는 추상 클래스 형태 혹은 이미 완성된 객체 형태로 존재하기 때문에 new를 사용하지 않는다
- 이미 만들어져있는 List정적 객체의 내부적인 팩토리 역활인 apply()가 동작하면서 새로운 List객체를 생성
- :: 는 리스트의 각 요소를 결합
- :::는 여러 리스트를 병합

```scala
val list1 ="a" :: "b" ::"c"::Nil
for(x <- 0 until list1.size)
println(list1(x))

val list2="d" ::"e"::Nil
val list0=list1 ::: list2
for(x <- 0 until list0.size){
println(list0(x))
}
```

- **Map**의 주요 기능
- 키를 통해 요소에 접근, 인덱스가 피룡하지 않다.

```scala
val list3="a"::"b"::"c"::Nil
val list4=1::2::3::Nil
val list5=2::2::4::Nil
println(list3 ++ list4)
println(list3.apply(0))
println(list3.reverse)
println(list4.max)
println(list4.min)
println(list4.sum)
println(list4.mkString(","))
println(list4.exists(a => 0>3))
println(list4.exists(_>3))
println(list4.contains(1))
println(list4.isEmpty)
println(list4.distinct)
```

```scala
val map=Map("number1" -> "aa",
            "number2" -> "bb",
           "number3" -> 3,
           5 -> "cc")
println(map.isEmpty) 
println("whole map:" +map)
println("keys:" +map.keys)
println("values: "+map.values)
println(map("number1"))
val map3= map + ("num4" -> 44)
println(map3)
val map2 =Map("n1"-> 100, "n2"-> 200)
val map4=map3 ++ map2
println(map4)
map4 - ("num4")
println(map4)
```

- **집합(set)**
- 중복되지 않는 값을 다뤄야 할때

```scala
var basket: Set[String] =Set()
basket+="딸기"
basket+="포도"
basket+="포도"
basket+="사과"
basket+="포도"
basket+="바나나"
println(basket)

var basket2: Set[String]=Set()
basket2 +="토마토"
basket2 +="당근"
basket2 +="감자"
basket2 +="사과"

println(basket.diff(basket2))
println(basket|baseket2)


```

- **튜플(tuple)**
- 여러 데이터를 하나의 묶음으로 처리하고 싶을 때 튜플로 처리
- 튜플은 N개의 데이터 쌍을 저장하는 자료 구조

```scala
val t1=(1,2) //튜플 생성
val t2=("a",1,"c")
val n1=t1._2//튜플 내용 참조 //2 결과값으로
val n2=t2._3// c 가 결과값으로
```

- **옵션**
- 여러 개의 값을 저장하는 자료 구조로서 값이 있을 수도 없을 수도 있는 경우에 사용
- None이거나 Some()을 하나 가지고 있다
- 맵에서 키를 이용해 값을 지겨울 떄, 해당하는 값이 있을 때는 Some()을 반환하고 없으면 None을 반환하여 로직에 사용
- 어떤 값이 들어 있으면 SOme 으로 래핑되어 있기 때문에  case Some()으로 패턴 매칭 할수 있고, 값이 없으면 None이기 떄문에 

```scala
object Ex{
    def main(args: Array[String]):Unit={
        val students=Map(
            1 -> "문진한",
            2 -> "엄다솔",
            3 -> "노순표"
           
        )
        val one=students.get(1)
        val four=students.get(4)
        
        println(one)
        println(four)
        println(one.get)
        println(four.getOrElse("값이 없습니다."))
    }
}
```

- **시퀀스**
- 내부적으로 인덱스에 대한 정보를 가지고 있고 인덱스와 관련해서 써야 할 기능이 많을 경우 쉽게 데이터를 다룰 수 있다.

```scala
val dounts: Seq[String] =Seq("Plain Dount", "Strawberry Dount","Glazed Dount")
println(s"Elements of dounts=$dounts")
println(s"Take elements from index 0 to 1=${dounts.slice(0,1)}")
println(s"Take elements from index 0 to 2=${dounts.slice(0,2)}")

```

- **이터레이터**
- hasNest와 next, length 등이 있다.

```scala
val list=List("a","b","c")
val i=list.iterator
while(i.hasNext)
println()
```

- **패턴매치**
- 스칼라에서는 패키지에 변수나 클래스 등을 선언 할 수 있다.
- 패키지 객체를 이용하면 Common과 같은 클래스를 정의하지 않고도 동일 패키지에서 사용하는 변수나 메서드 등을 공유 할 수 있다.
- package 키워드를 사용해 정의

```scala

```

- type은 새로운 타입을 선언하는 키워드
- 선언된 타입은 실제로 변수나 메서드의 타입으로 사용 가능
- 스칼라에서는 다른 클래스의 변수나 메서드 등을 사용하기 위해 import문을 사용
- 스칼라에서는 static키워드를 사용하지 않고, _를 사용해서 표기

- **함수 컴비네이터**
- 구현된 로직을 따라 컬렉션을 변형한 후 동일한 자료형의 컬렉션을 반환하는 역활을 맡는 메서드
- map(),foreach()컬렉션을 탐색하면서 그 안의 값을 바꾸는 역활
- map()리스트 자체를 변형하지 않고 List자료형을 반환 하면서 새로운 변수에 담게 한다
- foreach()아무값도 반환하지 않으며 리스트 자체의 값을 변형

```scala
val o=List(1,2,3,4)
println(o)
val n=o.map(i => i*10)
println(n)

val m=o.foreach(i => i*10)
```

- **filter(), filterNot()** -조건이 참, 거짓을 가릴 수 있는 형..?

```scala
val o =List(1,2,3,4)
println(o)
val n=o.filter(i => i => 3).map(i=>i*2)
println(n)
```

- **foldLeft(), foldRight()**컬렉션에 있는 여러 요소를 한쪽.

```scala
val o =List(1,2,3,4)
val sum=o.foldLeft(0.0)(_+_) 
println(s"Sum = $sum") //결과값은 10
```

- **partition()** 컬렉션을 나누는데 필요한 기능. 조건에 맞는 것들을 하나의 리스트로 저장하고 나머지 것을 다른 리스트에 저장

```scala
vol o=List(1,2,3,4)
val n=o.partition(i=> i<3)
println(n)
```

- **zip()** 두개의 리스트를 하나로 합친다.

```scala
val o=List(1,2,3,4)
val oo=List(5,6,7,8,9)
val n=o zip oo//<1,5), <2,6),(3,7),(4,8)
val nn=o:::oo//1,2,3,4,5,6,7,8,9
println(n)
println(nn)
```

- **find()** 원하는 조건에 맞는 첫 번째 요소를 반환

```scala
val o=List(1,2,3,4)
val n=o.find(i=> i>=2)
val nn=o.find(i=>i==9)
println(n)
println(nn)
```

- **drop(), dropWhile()**

```scala
val o=List(1,2,3,4,5,6,0)
val n=o.drop(4)//4 이하는 버린다.
val nn=o.dropWhile(i=> i<3)//조건식에 해당하는 것
println(n)
println(nn)
```

- **flattern()**리스트 안에 리스트가 중첩되어 있는 경우 풀어주는 기능을 수행

``` scala
val o=List(List(1,2,3,4),List(5,6))
val n=o.flatten
println(n)
```

```scala
val donuts1: Seq[String] =Seq("Plain","Strawberry","Glazed")
val donuts2: Seq[String]=Seq("Vanilla","Glazed")
val listDonuts: List[Seq[String]]=List(donuts1,donuts2)
val listDonutsFromFlatten:List[String]=listDonuts.flatten
```

- **예외 처리**
- try, catch, finally

```scala
import java.io.FileReader
import java.io.FileNotFoundException
import java.io.IOException
import java.io.Console

object Demo{
    def main(args: Array[String]): Unit={
        try{
            val f=new FileReader("input.txt")
            //읽는 것이 없기에 파일이 이써도 확인 못한다
            
        }catch{
            case ex: FileNotFoundException => {
                println("Missing file exception")
            }
            case ex: IOException => {
                println("IO Exception")
            }
        }finally{
            println("Exiting finally")
        }
    }
}
```

```scala
//파일 내용 읽기위해서
object Demo{
    def main(args: Array[String]){
        print("Please enter your input :")
        val line=Console.readLine
        println("Thanks, yout just typed: "+line)
    }
}
import scala.io.Source
object Demo{
    def main(args: Array[String]):Unit ={
        println("Following is the content read:")
        Source.fromFile("Demo.txt").foreach{
            print
        }
    }
}
```

```scala
//파일 내용 출력 하기 위해서
import scala.io.StdIn.readLine
import java.io.File
import java.io.PrintWriter


object Ex{
    def main(args: Array[String]):Unit ={
        val fileName="test.txt"
        var input=readLine("파일에 쓸 내용 입력 하세요!")
        
        val writer=new PrintWriter(new File(fileName))
        writer.write(input)
        writer.close
        //출력파일은 패키지 디렉토리에 생성된다.
        print( "입력하신 텍스트를 " + fileName+"에 저장했습니다.")
    }
}
```

- **either**
- 둘 중 하나를 선택

```scala
object Ex {   
   def main(args: Array[String]): Unit = {
       val input = scala.io.StdIn.readLine("값을 입력하세요:")
       val result: Either[String, Int] = try {
          Right(input.toInt)
       } catch {
         case e : Exception => Left(input)
       }
       print(result.getClass)             
   }
}

```





# 스파크

## 1. 정의

오픈소스 병렬분산 처리 플랫폼으로 스트림처리와 머신러닝을 효과적으로 수행하기 위함이다.

#### 1.1 스파크 구조

복수의 컴포넌트로 구성 

1. sql처리
2. 스트림처리
3. 머신러닝
4. 그래프 처리(그래스X)

***

## 특징

1. 메모리 기반(하둡은 디스크 기반이라 스파크가 더 빠르다.)
2. 대용량 고속 처리 엔진
3. 범용 분산 클러스터 컴퓨팅 프레임워크



***

## 설치하기

1. https://www.apache.org/dyn/closer.lua/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz 에서

   http://apache.tt.co.kr/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz 클릭해서 다운

```cmd
su -
cd usr/local
tar -zxvf /home/hadoop/Downloads/spark-2.4.3-bin-hadoop2.7.tgz
ls -l

ln -s spark-2.4.3-bin-hadoofp2.7 spark #심볼릭링크 만들기
ls -l
chown -R hadoop:hadoop spark
ls -l
```

 2. 

```cmd
su hadoop
vi .bash_profile
#설정 저장 위치는 
export SPARK_HOME=/usr/local/spark
export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
# export PATH=$PATH:$JAVA_HOME 에 추가해야한다
/bin:$SPARK_HOME/          # 애는 잠시 지운다. bin:$SBT_HOME/bin
PATH=$PATH:$HOME/.local/bin:$HOME/bin

export JAVA_HOME=/usr/local/jdk1.8.0_221
export HADOOP_HOME=/usr/local/hadoop-2.7.7
export HADOOP_CMD=/usr/local/hadoop-2.7.7/bin/hadoop
export HADOOP_STREAMING=/usr/local/hadoop-2.7.7/share/hadoop/tools/lib/hadoop-streaming-2.7.7.jar
export HIVE_HOME=/usr/local/hive
export SPARK_HOME=/usr/local/spark
export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin:
#path가 밑에잇어야 한다. 그러므로 path 위에 export를 추가하자
source .bash_profile
```



#### 클러스터 구성의 스파크 동작 확인

리눅스의 하둡계정을 로그아웃했다가 로그온 해주자~

```cmd
#hadoop 계정에서
spark-shell --master local verbose
scala> #로 들어가진다.
```



#### 실행해보자

```scala
//로컬 파일시스템에서 파일을 읽어들여서 RDD로 생성
scala> val file=sc.textFile("file:///usr/local/spark/README.md")
//RDD로부터 한 행(라인)단위로 처리-단어 분리 후 RDD 저장 생성
scala> val words=file.flatMap(_.split(" "))
//여기까지가 변환처리다

//같은 단어끼리 모아서 요약(개수) 계산-map형태로 단어와 출현횟수
scala> val result=words.countByValue

scala> result.get("For").get

```



## sbt 설치하기

스파크 애플리케이션은 소스코드를 컴파일 하고 JAR파일로 패키징해야 하는데 이를 해주는 것이 sbt이다. sbt는 스칼라와 자바로 기술된 소스코드를 통합 관리하기 위한 툴이다.(패키지 작성, 애플리케이션 개발 프로젝트 빌드 프로세스 관리 등 )

https://www.scala-sbt.org/download.html  sbt 1.2.7.tgz를 받자

```cmd
#root 계정(su -)
[root@master ~]# tar -zxvf /home/hadoop/Downloads/sbt-1.2.7.tgz -C /opt/
[root@master ~]# ls -l /opt
# 소유자가 hadoop이라 안 바꿔도 된다.


[hadoop@master ~]$ vi .bash_profile

export SBT_HOME=/opt/sbt
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin:$SBT_HOME/bin
[hadoop@master ~]$ source .bash_profile
#로그아웃 했다가 로그온!

[hadoop@master ~]$ sbt about
#로 정보 확인
```



#### sbt로 스파크 애플리케이션 작성

1. sbt 프로젝트 디렉토리 구조 만들기

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir  spark-simple-app

#소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ cd spark-simple-app/
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala

#sbt 설정 파일 저장 디렉토리
[hadoop@master spark-simple-app]$ mkdir project

#소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master spark-simple-app]$ cd src/main/scala/lab/spark/example/
# 뒤의 새게의 파일이 바로 package명이다!
[hadoop@master example]$ vi SundayCount.scala

```

#### SundayCount.scala 작성

```javascript
package lab.spark.example

import org.joda.time.{DateTime,DateTimeConstants}
import org.joda.time.format.DateTimeFormat
import org.apache.spark.{SparkConf,SparkContext}

object SundayCount{
def main(args: Array[String]){
if (args.length<1){
throw new IllegalArgumentException(
"명령 인구 경로")
}

val filePath=args(0)
val conf=new SparkConf
val sc=new SparkContext(conf)

try{
val textRDD=sc.textFile(filePath)
val dateTimeRDD=textRDD.map{dateStr=>
val pattern=
DateTimeFormat.forPattern("yyyyMMdd")
DateTime.parse(dateStr,pattern)ㅗㅝㅓ
}
val sundayRDD=dateTimeRDD.filter{dateTime=>
dateTime.getDayOfWeek==DateTimeConstants.SUNDAY
}
val numOfSunday=sundayRDD.count
println(s"주어진 데이터에는 일요일이${numOfSunday}개 들어 있습니다.")
}finally{
sc.stop()
}
}
}

```

#### build.sbt 작성



```cmd
[hadoop@master ~]$ cd ~/spark-simple-app
[hadoop@master spark-simple-app]$ vi build.sbt

name :="spark-simple-app"
version :="0.1"
scalaVersion:="2.11.12"
libraryDependencies ++=
Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided",
"joda-time" % "joda-time" %"2.8.2")
assemblyOption in assembly :=(assemblyOption in assembly).value.copy(includeScala=false)

```



#### plugins.sbt작성과 sbt-assembly이용에 필요한 설정

```cmd
[hadoop@master spark-simple-app]$ cd project
[hadoop@master project]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n"%"sbt-assembly"%"0.14.10")
# 저장하고 나오자~
[hadoop@master project]$ cd ~/spark-simple-app/
[hadoop@master spark-simple-app]$ sbt assembly

```

Packaging /home/hadoop/spark-simple-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar  여기에 jar가 생겼다.

jar로 패키지 한 것!

#### jar파일 실행해보자. 

1. 먼저 date.txt를 만들고 하둡에 올린다!

```cmd
[hadoop@master spark-simple-app]$ cd ~
[hadoop@master ~]$ vi date.txt

20150322
20150331
20150417
20150426
20150506
20150523
20150524
20150712
20150727
20150810
20150830
20150927

# 작성 (공백없이 작성해달라)


[hadoop@master ~]$ hadoop fs -mkdir /data/spark/
[hadoop@master ~]$ hadoop fs -put date.txt /data/spark
[hadoop@master ~]$ hadoop fs -ls /data/spark
Found 1 items
-rw-r--r--   1 hadoop supergroup        116 2019-08-27 13:37 /data/spark/date.txt
# 확인하자


[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.SundayCount --name SundayCount ~/spark-simple-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/spark/date.txt

#"" 오류가 생겼다면 date.txt의 공백을 다 지운상태에서 하둡의 파일을 지우고 다시 -put 하자~ 그리고 다시 assembly
```



### 스파크 WordCount 예제

1. SparkContext생성
2. (입력 소스로부터) RDD 생성
3. RDD 처리
4. 결과 파일 처리
5. SparkContext종료

```scala

```



## 스파크 셀

1. Spark Context
   - 애플리케이션과 스파크 클러스터와의 연결을 담당
   - 모든 스파크 애플리케이션은 SparkContext를 이용하여 RDD나 accumulator또는 broadcase변수 등을 다룬다.
   - 스파크 애플리케이션을 수행하는 데 필요한 각종 설정 정보를 담는다.
2. RDD 생성
3. 스파크 클러스터
4. 분산 데이터로서 RDD
5. 파티션

### SparkContext

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir wordcount-app

[hadoop@master ~]$ cd wordcount-app

# 소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala  
#sbt 설정 파일 저장  디렉토리 생성
[hadoop@master ~]$ mkdir project

# 소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master ~]$ cd  src/main/scala/lab/spark/example
[hadoop@master ~]$ vi WordCount.scala
#---------------------------------------------------------------------------------------------
package lab.spark.example

import org.apache.spark.{SparkConf,SparkContext}

object WordCount{
def main(args: Array[String]){
require (args.length>=1,
"드라이버 프로그램의 인수에 단어를 세고자 하는"+
"파일의 경로를 지정해 주세요.")

val conf=new SparkConf
val sc =new SparkContext(conf)

try{
val filePath=args(0)
val wordAndCountRDD = sc.textFile(filePath)
.flatMap(_.split("[ ,.]"))
.filter(_.matches("""\p{Alnum}+"""))
.map((_, 1))
.reduceByKey(_+_)

wordAndCountRDD.collect.foreach(println)
}finally{
sc.stop()
}
}
}

#---------------------------------------------------------------------------------------------
[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ vi build.sbt

name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)


[hadoop@master ~]$ cd project
[hadoop@master ~]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")


#어플리케이션 빌드
[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ sbt assembly

#데이터 소스 생성
[hadoop@master ~]$ vi simple-words.txt
cat
dog
.org
cat
rabbit
bear
cat
&&
tiger
dog
rabbit
100
bear
tiger
cat
rabbit
?bear

#하둡 파일 시스템에 simple-words.txt파일 업로드
[hadoop@master ~]$ hadoop fs -mkdir  /data/wordcount
[hadoop@master ~]$ hadoop fs -put simple-words.txt  /data/wordcount


[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.WordCount --name WordCount ~/wordcount-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/wordcount


```

#### 스파크 RDD 

- **cogroup**
- RDD의 구성요소가 키와 값의 쌍으로 이루어진 경우 사용 가능

```scala
var rdd=sc.parallelize(List(("k1","v1"),("k2","v2"),("k1","v3")))
var rdd2=sc.parallelize(List("k1","v4"))
var result=rdd.cogroup(rdd2)
result.collect.foreach{
    case(k, (v_1,v_2))=> {
        println(s"($k,[${v_1.mkString(",")}],[$(v_2.mkString(", "))])")
    }
}
```



- **distinct()**
- RDD원소에서 중복을 제외한 요소로만 구성

```scala
var rdd=sc.parallelize(List(1,2,3,1,2,3,1,2,3))
var result=rdd.
```



- **substract()**
- 차집합

```scala
val rdd1 = sc.parallelize( List("a", "b", "c", "d", "e"))
val rdd2 = sc.parallelize( List("d", "e"))
val result = rdd1.substract(rdd2)
println(result.collect.mkString(", "))

```

- **intersecton()**
- 두개의 RDD가 있을 때 rdd1과 rdd2에 동시에 속하는 요소로 구성된 새로운 RDD를 생성하는 메서드
- 결과로 생성되는 RDD에는 중복된 원소가 존재하지 않는다.

```scala
val rdd1 = sc.parallelize( List( "a", "a", "b", "c"))
val rdd2 = sc.parallelize( List( "a", "a", "c", "c"))
val result = rdd1.intersection(rdd2)
println(result.collect.mkString(", "))

```

- **leftOuterJoin(), rightOuterJoin()**
- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용 가능
- 조인 결과를 표시할 떄는 값이 존재하지 않는 경우를 고려해 Option타입을 이용

```scala
val rdd1 = sc.parallelize( List( "a", "a", "b", "c")).map((_, 1))
val rdd2 = sc.parallelize( List( "b", "c")).map((_, 2))
val result1 = rdd1.leftOuterJoin(rdd2)
val result2 = rdd1.rightOuterJoin(rdd2)
println("Left:" + result1.collect.mkString("\t"))
println("Right:" + result2.collect.mkString("\t"))

```

- **substractByKey()**
- rdd1의 요소 중 rdd2에 같은 키가 존재하는 요소를 제외한 나머지로 구성된 새로운 RDD

```scala
val rdd1 = sc.parallelize( List("a", "b")).map(_, 1))
val rdd2 = sc.parallelize( List("b" )).map((_, 2))
val result = rdd1. substractByKey(rdd2)
println(result.collect.mkString("\n"))

```

- **reduceByKey()**
- RDD의 구성요소가 키와 값으로 구성된 경우에 사용 가능
- 같은 키를 가진 값들을 하나로 병합하여 키-값 쌍으로 구성된 새로운 RDD생성

```scala
var rdd = sc.parallelize( List( "a", "b", "b")).map((_, 1))
var result = rdd.reduceByKey(rdd)
println(result.collect.mkString(","))

```

- **foldByKey()**

```scala
val rdd = sc.parallelize( List( "a", "b", "b")).map((_, 1))
val result = rdd.foldByKey(_+_)
println(result.collect.mkString(","))

```

- **combineBykey()**
- 값을 병합하기 위해 사용
- 파티션 단위로 사용 가능

```scala
def  reduceByKey(func: (V, V) => V) : RDD[(K, V)]
def  foldByKey(zeroValue: V)(func: (V, V) => V) : RDD[(K, V)] 
combineByKey[C](createCombiner:(V)=>C, mergeValue:(C, V)=>C, mergeCombiners: (C, C) => C):RDD[(K, C)]

```



```scala
//콤바이너 역활을 할 Record클래스 정의
case class Record(var amount: Long, var number: Long=1){//메서드 파라미터에서 기본값 설정
    def map(v:Long)=Record(v)
    def add(amount:Long):Record={
        add(map(amount))
    }
    def add(other:Record):Record={
        this.number+=other.number
        this.amount += other.amount
        this
    }
    override def toString =s"avg:${amount/number}"
}
//combineByKey()를 이용 한 평균값 계산
val data =Seq(("Math",100L),("Eng",80L),("Math",50L),("Eng",60L),("Eng",90L))
val rdd=sc.parallelize(data)
val createCombiner=(v:Long)=>Record(v)
val mergeValue =(c:Record,v:Long)=>c.add(v)
val mergeCombiners=(c1:Record,c2:Record)=>c1.add(c2)
val result=rdd.combineByKey(createCombiner,mergeValue,mergeCombiners)
println(result.collect.mkString("\n"))

(Math,avg:75)
(Eng,avg:76)


```









