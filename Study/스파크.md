

https://wikidocs.net/book/2350 scala참고!

## Scala

스칼라는 2004년 마틴 오더스키(Martin Odersky)가 발표한 **객체 지향 언어의 특징과 함수형 언어의 특징을 함께 가지는 다중 패러다임 프로그래밍 언어**다.

***

## 특징

1. 객체지향 언어의 특성 +함수형 언어의 특성(자바스크립트랑 파이썬, R도 비슷)
2. 자바가상머신(JVM)에서 동작하는 JVML언어
   - 자바가상머신위에서 동작하는 것은 scala,kotlin,groovy등
3. 자바 가산머신위에서 동작하기에 자바의 모든 라이브러리 사용 가능 
4. 스칼라 컴파일러를 통해 스칼라 코드로 변환하고, 바이트 코드는 JVM상에서 자바와 동일하게 실행(즉 플랫폼에 독립적이다, Windows나 Linux나 Mac에  동일하게 실행)

### 함수형 언어

1. 함수형 언어 특성으로 자바에 비해 코드 길이가 짧다.

   - **적은 양의 코드로 방대한 규모의 시스템을 작성할 수 있다.**
   - 스칼라에서는 모든것이 Object이기 때문에 ==로 모든 비교 가능

2. 객체지향 프로그래밍 언어와 함수형 프로그래밍 모두 완벽하게 지원

   - 모든 것이 객체이면 함수가 first object(함수적 프로그래밍 특성은 함수를 마치 하나의 값으로 취급하여 변수 또는 파라미터로 넘길 수 있다.)
   - 모든 것을 함수로 해결하면 의도하지 않는 동작이 발생 할 일이 없고, 한번 검증된 함수는 신뢰할 수 있기 때문에 버그가 줄어든다.
   - Immutable 변수는 문제를 단수화 시켜주어 데이터 공유, 병렬처리에 강하다.

   #### 함수 언어

   > 함수의 실행이 외부에 영향을 끼치지 않는 함수. 

   #### 익명 함수

   > 선언부가 없는 함수. 코드의 길이를 줄일 수 있다.
   >
   > Arrays.asList(1,2,3).stream().reduce((a,b) -> a-b).get();
   >
   > 여기서 reduce가 익명함수이다.

   #### 고차 함수

   > 함수를 인수로 취하는 함수. 함수를 입력 파라미터나 출력 값으로 처리 가능
   >
   > 

### 장점

1. 코드의 직관성, 신축성
2. 풍부한 표현식과 연산자
   - Fist-class function
   - Closure
3. 간결함
   - 타입 추론
   - 함수 리터럴(Literal)
4. 자바와의 혼용가능 객체지향+함수형 언어
   - 자바 라이브러리 재사용 가능
   - 자바 도구를 재사요 ㅇ가능
   - 성능의 손실 없이 사용 가능
   - 스칼라에서는 모든 것이 객체
5. 동시성에 강한 언어
   - 스칼라에서는 많은부분이 변경불가능 속성을 가지게끔 되어있다.
   - 아카(AKKa)라이브러리=동시성 프로그래밍에서 뛰어난 액터(=**함수**) 모델이용
   - 액터 모델은 각각의 액터가 서로 간의 메시지를 통해서만 의사소통을 하고  액터를 이루는 각종 변수나 속성은 서로 공유하지 않는다.
6. expression( 결과를 반환하는 문장) 중심 언어=표현중심언어
7. 필요할 때 implicit 예약어를 사용하면 명시적인 표현을 감출 수있다.

## Scala설치

1. 윈도우에서 해도 된다.
2. https://www.scala-lang.org/download/ (혹은 구글에서 스칼라 다운로드 검색) [scala-2.13.0.msi](https://downloads.lightbend.com/scala/2.13.0/scala-2.13.0.msi) 다운
3. Scala IDE-IntelliJ, or sbt,Scala's build tool. Http://scala-ide.org 
4. 이클립스는 마켓플레이스에서 스칼라 IDE를 설치하면 슼라라 프로젝트 생성 가능 scala ide 검색

#### 실행

1. cmd 창을 열고
2. scala 입력
3. prompt 가 열린다. (스칼라 셀은 스칼라 )
   - 안되는 경우 환경변수 추가 하자 
   - Path에서 `C:\Program Files (x86)\scala\bin` 추가

#### 기초 

```cmd
scala>1
scala>var a=1
scala>val b=1
scala>b=2 #하면 오류생긴다(val은 불변 변수 선언이므로)
scala>val c:Int=0 #변수 타입 지정
scala>1 to 10
sclaa>(1 to 10).toList
scala>1.toDouble #형변환
scala>1.0.toInt #형변환
```

- 스칼라의 기본 변수 타입은 모두 클래스
- 변수 선언 var,val(불변 변수 선언),변수 타입 생략 가능
- Range 타입 -1 to 10, 1 to 10 by 2 , 리스트나 배열 타입으로는 형변환 해야 한다.

```cmd
scala>:help
:edit
:help
:history
:load <path>
:quit
:save <path>
```

```scala
object Ex1{
def main(args: Array[String]): Unit={
println("Hello, Scala!")
}
}
```

메모장에 제목 "Ex1.scala" 로 저장

실행은 cmd 창에서 저장된 위치로 가서 

```cmd
C:\scala> scalac Ex1.scala # C폴더 아래 scala라는 파일을 생성한 상태, scalac 로 컴파일 
C:\scala> dir/w #파일 확인
C:\scala>scala Ex1
Hello, Scala!
#실행확인

```



- null보다는 none을 추천

- var는 언제든지 값이 바뀔 수 있는 일반적인 변수 선언

- val는 final변수 선언

- 변수 의 값으로 null이나 None으로 초기화 가능

- 컴파일러가 알아서 자료형에 대해 판단하고 필요하면 묵시적 형 변환을 통해 필요한 자료형으로 바꿔준다.

- 자료형을 명시적으로 선언하여 해당 자료형으로 값이 저장

- 기본 자료형은 자바에서 파생문자열인 String을 제외하고 AnyVal이라는 공통의 이름으로 불리며, 참조 자료형은 AnyRef로 불린다.

  ```cmd
  scala>var d:Int=0.1
  scala>var e=null
  scala>d=None
  scala>var f:Boolean=1 #에러가 남으로 호환이 안된다.
  
  scala>val str2="""a
  b
  c"""
  #멀티 라인 문자열은 세개의 쌍따옴표를 이용
  ```



#### 접두어

- 접두어를 이용한 처리

- 접두어 S

- ${변수명}을 이용하여 문자열 안의 변수를 값으로 치환

  ```cmd
  scala> val name="David"
  scala>println(s"Hello! ${name}") 
  sclala>println("${1+1}")
  scala>println(s"${1+1}")
  ```

- 접두어 F

- 문자열 포맷팅 처리 , 자바의 pirnf()와 같은 방식

  ```scala
  val height:Double=182.3
  val name="james"
  //f접두어를 이용한 값 변화 테스트
  println(f"$name%s is $height%2.2f meters tall")
  ```

- 접두어 raw

- 접수어 raw는 특수 문자를 처리하지 않고 원본 문자로 인식

  ```scala
  s"가\n나" //\n으로 개행 문자 처리(엔터)
  raw"가\n나"//\n을 문자 그대로 인식
  ```

##### 접두어 예제

```scala
var str3=s"println $str1"
println(str3)
println(s"2*3=${2*3}")
def minus(x: Int, y:Int)=x-y
println(s"${Math.pow(2,3)}") //2를 3번 곱한것 (지수승)
println(s"${minus(2,3)}")
```

#### Range타입

1. Range 타입 -1 to 10, 1 to 10 by 2, 리스트나 배열 타입으로는 형변환 해야한다

2. type 예약어는 자료형이 복잡한 경우 변칭을 주어 쓸 수 있게 한다.

   ```scala
   type Name=String
   type Person=(String,Int)
   type Ftype=String=>Int //함수 표현식
   val name: Name="홍길동"
   val person:Person=("korea",24)
   val f:Ftype=text=>text.toInt
   println(f) //객체 생성 됐따는 것
   f("34.125")//에러
   f(34.125)//에러 
   f(34) //에러
   f("34")//Int라 뜬다
   ```

#### 조건문, 반복문

- 조건문 : if /else
- 반복문 :for, while, do while

```scala
for(x<-1 to 10){
    //반복할 실행문
    println(x)// 10까지 나온다.
}
for(x<-1 range){
    //반복할 실행문
    println(x)
}
for(x<-1 until 10){
    //반복한 실행문
    println(x) //9까지 나온다.
}
//조건이 있는 반복문
for(i <-10) if (i % 2==0){
    println(i)
}
for(i <- 1 to 10) if (i % 2==0){
    println(i)
}
val lst=(10 to 100 by 10).toList
//인덱스가 있는 for ans
for((num,index) <- lst.zipWithIndex){
    println(s"index: $num")
}
```

- 이중 for 문

```scala
object Ex{
    def main(args: Array[String]): Unit={
        for(x <- 1 until 5; y <- 1 until 5){
            print(x + "*" + y + "=" + x*y +"|")
        }
    }
} //저장해서 scalac 하거나 for문만 cmd에서 scala에 실행
```



#### 함수

- 변수와 마찬가지로 :를 이용해 반환 자료형을 정의 , 반환 자료형이 함수의 자료형을 결정
- Unit은 자바의 void 자료형과 같다(반환 결과가 없는 함수에 붙는 자료형)
- 반환 값이 있을 때도 반환 자료형 생략 가능
- 명시적으로 return을 사용하는 경우, 함수 선언하는 곳에도 반환 자료형을 명시
- 반환 자료형을 명시한 경우 다른 자료형을 반환하려고 하면 에러 발생

```scala
def 함수([매개변수]): [반환 자료형]= {
//구현할 로직
}
```

```scala
object Ex{
def main(args: Array[String]): Unit = {
println("반환받은 값: "+ name())
}
def name()={
val a=10
a
}
}
```

```scala
def name() :  Int={
val a=10
return a
}
```

```scala
def name() = {
val a=10
return a
} //error :method name has return statement; needs result type
```

```scala
def name3(): Int={
val a=10
a
}
```

- 스칼라에서는 함수 인자의 타입을 명시해야한다

- 인자가 없는 함수의 경우 호출시 괄호 생략 가능

- ```scala
  def addOne(m: Int):Int =m+1
  val three =addOne(2)
  def three()=1+2
  three()
  three
  ```

- **이름없는 함수**를 만들 수 있다.

- 이름 없는 함수르 ㄹ다른 함수나 식에 넘기거나 val에 저장 가능

- 함수가 여러 식으로 이루어진 경우 {}를 사용해 이를 위한 공간 만들 수 있다.

- ```scala
  (x:Int) => x+1
  인터프리터가 부여한 이름(1)
  
  val addOne=(x:Int)=> x+1
  addOne(1)
  ```

- ```scala
  def timesTwo(i:Int):Int={
  println("hello world")
  i*2
  }
  timesTwo(2)
  
  {i:Int=>
  println("hello world")
  i*2} //즉시 실행 함수, 익명함수
  ```

- **(_) 사용하기**

- 함수 호출시 (_)사용하면 일부만 적용 가능, 그렇게 하면 새로운 함수 얻는다. 스칼라에서 밑줄은 문맥에 따라 의미가 다르다.

- 보통 이름없는 마법의 문자!

- 인자 중에서 원하는 어떤 것이든 부분 적용 가능, 위치는 아무곳이나 가능

- ```scala
  def adder(m: Int, n:Int) =m+n
  val add2=adder(2,_:Int)
  add2(3)
  ```

- **커리함수**

- 함수의 인자중 일부 적용하고, 나머지는 나중에 적용하게 남겨두는 것

- ```scala
  object Ex{
      def main(args: Array[String]): Unit={
          val thisYear = 2009
          val fixedValueFunction=go(thisYear, _:String)
          fixedValueFunction("test1")
          fixedValueFunction("test2")
          fixedValueFunction("test3")
      }
      def go(thisYear:Int, String:String)={
          println(String + ":" + thisYear)
      }
  }
  ```

- **=>**를 이용한 함수 표현식

- 스칼라 컴파일러는 = >표현식을 보면 function객체로 선언

- 스칼라는 매개변수가 하나면 function1, 두개면 function2,...22개 까지 function을 상속 하는 트레이트 제공

- ```scala
  def functionAsValue=(y:Int)=> y+10
  //컴파일러가 아래로 컴파일 한다
  val functionAsValue : Int=>Int=new Function(Int,Int){
      def apply(y:Int):Int=y+10
  }//컴파일러가 이리 해주기에 우리는 위의 한줄로만 써도 된다.
  ```

- **변수가 함수 넣기**

- 명시적으로 함수가 기대되지 않는 곳에서 =연산자를 이용해 매개변수가 필요한 함수를 대입하였을때 에러가 발생-객체화되어 있찌 않는 함수를 바로  val에 대입하면 에러 발생

- ```scala
  Object Ex{
      def main(args: Array[String]): Unit={
          val g=f_
          println(f(1))
      }
      def f(i:Int)=i
  }//에러발생?
  ```

- ```scala
  val g(Int=>Int)=f//이리 써야 한다?
  ```

- ```scala
  val g=f //는 그대로 두고
  //f 를 선언할 때 =>를 이용
  def f=(i:Int)=>i
  ```

- **재귀함수**

- ```scala
  object Ex{
      def main(args: Array[String]): Unit={
          val result=calc(x => x*x, 2,5)
          println(result)
      }
      def calc(f:Int = > Int, start: Int, end: Int) = {
          //합계를 구하는 재귀 함수
          def loop(index:Int, sum:Int):Int={
              if(index>end) sum//인덱스가 끝 값보다 크면 sum을 반환
              else loop(index+1, f(index)+sum)
          }
          loop(start,0)//루프를 최초 호출
      }
  }
  ```

- **매개변수가 여러 개인 함수**

  ```scala
  object Ex{
      def printlnStrings(args: String*)={
          for(arg <- args){
              println(arg);
          }
      }
  }
  
  printlnStrings("st1","st2","st3")
  printlnStrings( )
  
  val array1=Array("1","2","3","4")
  printlnStrings(array1) //String을 요구하는데 Array[String]가 들어가 오류 뜬다. 
  ```

-  **매개변수의 기본값 설정**

  ```scala
  object Ex{
      def default(a: Int=4,b:Int=5):Int=a+b
      println("기본값은"+default())
      println("변수값은"+default(11,6))
  }
  ```

- **apply**

- 매번 매서드 이름을 적는 것을 피하기 위해 사용

- 변수를 받아 함수에 적용시켜 결과를 만들어내는 설정자와 같은 역활

- Apply() 를 이용하면 생성자처럼 초기화하거나 클래스 안에 특정한 메서드를 기본 메서드로 지정하는 것을 편하게 할 수 있따.

  ```scala
  object Ex{
      class SomeClass{
          def apply(m:Int)=method(m)
          def method(i:Int)={
              println("method(Int) called")
              i+i
          }
          def method2(s:String)=5
      }
      val something=new SomeClass
      println(somethin(2))
  }
  ```

- **암묵적 형변환**

- Implicit 는 에러는 바로 내지 않고 해당하는 함수가 있으면 그것을 사용해서 암묵적으로 형 변환을 도와주어 함수의 활용도를 높힌다.

  ```scala
  object Sample{
  case class Person(name:String)
  //implicit def StringToPerson(name:String) :Person=Person(name) 이게 없으면 sayHello가 형변환이 안되고 이 게 있으면 자동 형변환된다
  def sayHello(p:Person): Unit ={
  print("Hello,"+p.name)
  }
  sayHello("korea")
  }
  ```

- 반환 자료형과 매개변수만으로 판단하기 때문에 에러 발생 할 수 있다.

  ```scala
  implicit def doubleToInt(d: Double)=d.toInt
  implicit def doubleToInt2(d: Double)=d.toInt+1
  val x:Int =18.0//에러 발생
  
  def doubleToInt(d:Double)=d.toInt
  val x:Int=doubleToInt(18.0)
  
  def doubleToInt(d:Double)=d.toInt
  val x:Int=18.0//에러 발생
  
  implicit def doubleToInt(d:Double)=d.toInt
  val x:Int =18.0
  ```

- **객체**

- 스칼라에서는 연산자와 메서드를 포함한 모든  것이 객체

- 객체 생성 방법

  - 클래스를 통한 인스턴스 화 -new 를 사용하여 계속 인스턴스 생성 가능
  - object예약어를 통해 객체 생성-싱글턴 객체로서 유일한 객체 생성

- 클래스

  - 클래스 안에서 메소드는 def로, 필드는 val로 정의한다. 메소드는 단지 클래스(객체)의 상태를 접근할 수 있는 함수에 지나지 않는다

- 스칼라에서는 public class대신 object예약어를 통해 처음부터 메모리에 객체 생성하고 컴파일러는 객체에 main이라는 이름이 있으면 main을 프로그램의 시작점으로 컴파일 한다.

  ```scala
  object Ex{
      def main(args: Array[String]): Unit={
          val apple=new Fruit("사과")
          println(apple.name)
      }
  }
  clss Fruit(input:String){
      var name=input
  }
  ```

- **케이스 클래스**

- 자동으로 멤버 변수를 만들어 주고, 외부에서도 멤버 변수에 접근이 가능하도록 한다. toString,hashCode,equals를 자동으로 만들어 준다.

  ```scala
  case class Fruit2(name:String)
  val apple=new Fruit2("사과")
  println(apple.name)
  val apple2=new Fruit2("사과")
  println(apple2.name)
  
  println(apple2.equals(apple)) //true
  println(apple.hashCode)
  println(apple.toString)//Fruit2(사과) 로 결과가 나온다.
  ```

- 스칼라에서는 생성자가 특별한 메소드를 따로 존재하지 않는다. 클래스 몸체에서 메소드 정의 부분밖에 있는 모든 코드가 생성자 코드가 된다.

  ```scala
  class Calculator(brand:String){
      //생성자
      val color:String=if(brand=="TI"){
          "blue"
      }else if(brand=="HP"){
      "black"
  }else{
      "white"
  }
  
  //인스턴스 메소드
  def add(m:Int, n:Int):Int=m+n
  }
  val calc=new Calculator("HP")
  calc.color
  ```

- 상속과 메소드 Overloading

  ```scala
  //상속
  class ScientificCalculator(brand:String) extends Calculator(brand){
      def log(m: Double,base:Double)math.log(m) / math.log(base)
  }
  
  ```

- **추상클래스 ** 는 메소드 정의는 있지만 구현은 없는 클래스이다. 대신 이를 상속한 하위클래스에서 메소드를 구현하게 된다. 추상 클래스의 인스턴스를 만들 수는 없다.

```scala
abstract class Shape{
    def getArea():Int
}
class Circle(r:Int)extends Shape{
    def getArea():Int ={r*r*3}
}
val s=new Shape //abstract이므로 변경 불가능
val c=new Circle(2) //
```

- **트레잇(trait)**
- 특성 : 하나의 완성된 기능이 아닌 어떤 한 객체에 추가될 수 있는 부가적인 특성
- 클래스의 부가적인 특성으로 동작, 자체로 인스턴스화는 가능하지 않다.
- 다른 클래스가 확장(즉, 상속) 하거나 섞어 넣을 수 있는(이를 믹스인 Mix in 이라 한다.)필드와 동작의 모음
- 클래스는 여러 트레잇을 with키워드를 사용해 확장
- 자유롭게 변수 선언하고 로직을 구현

```scala
trait Car{
    val brand: String
    
}
trait Shiny{
    val shineRefraction:Int
}
/*class BMW extends Car{
    val brand="BMW"
}*/
calss BMW extends Car with Shiny{//with로 확장
    val brand="BMW"
    val shineRefracton=12
}

//혹은
case class Car(brand:String)
case class Shiny(shineRefraction:Int)

```

- 추상클래스 대신 트레잇을 사용해야 하는 경우
- 인터페이스 역확을 하는 타입을  설계할때 트레잇과 추상클래스 두 가지 다 어떤 동작을 하는 타입을 만들수 있으며, 확장하는 쪽에서 일부를 구현하도록 요청
- 클래스는 오직 하나 만 상속 가능, 트레잇은 여러가지 상속 가능
- **트레잇 쌓기**
- 여러개를 한 클래스에서 상속받는데 상속에 문제가 생길경우(메소드를 호출했는데 다양한 클래스에 같은 메소드로 인한 충돌 발생) override 예약어와 함꼐 적당한 상속 관계를 만들 수 있다.
- 최종적인 상속받은 클래스의 메서드가 수행되도록 한다.

```scala
abstract class Robot{
    def shoot="뿅뿅"
}
trait M16 extends Robot{
    override def shoot="빵야"
}
trait Bazooka extends Robot{
    override def shoot="뿌왕뿌왕"
}
trait Daepodong extends Robot{
    override def shoot="콰르르르르를ㅇ"
}
class Mazinga extends Robot with M16 with Bazooka with Daepodong 
val robot=new Mazinga
println(robot.shoot)
//"콰르르르르르를ㅇ"이 출력된다.
```

- 모두의 기능을 실행하도록 상위클래스 super를 호출해서 해당하는 메서드를 실행되게 할 수 있다.

```scala
abstract class AnotherRobot{
    def shoot="뿅뿅"
}
trait M16 extends AnotherRobot{
    override def shoot=super.shoot + "빵야"
}
trait Bazooka extends AnotherRobot{
    override def shoot=super.shoot + "뿌왕뿌왕"
}
trait Daepodong extends AnotherRobot{
    override def shoot=super.shoot + "콰르르르르를ㅇ"
}
class Mazinga extends AnotherRobot with M16 with Bazooka with Daepodong 
val robot=new Mazinga
println(robot.shoot) //하면 모든것이 합쳐져서 나온다.
```

- **Compaion Object** static이 필요한 경우 대신 사용 가능
- **패턴 매치**
- 기본형이 아닌 튜플을 사용하는 경우 튜플 형식으로 변수를 정의해야 매칭된다.
- 리스트의 경우도 각 위치에 해당하는 값이 변수로 할당된다.
- 케이스 클래스의 경우는 클래스 형태를 그대로 사용해서 속성 정보를 매칭 할 수 있다.

```scala
val b=1
b match{
    case v if v==1 => println("b")
    case _ => println("err")
}
val c=(a,b)
c match{
    case(c1,c2) => println("c1:" + c1)
}
val d=List(1,3,5)
d match{
    case e1::e2::xs => 
}
```

```scala
def matchFunction(input:Any): Any =input match{
    case 100 => "hundred"
    case "hundred" => 100
    case etcNumber: Int => "입력값은 100이 아닌 Int형 정수입니다."
    case _ => "기타"
}
println(matchFunction(100))
println(matchFunction("hundred"))
println(matchFunction(1000))
println(matchFunction(1000.5))
println(matchFunction("thousand"))
```

```scala
case class Person(name:String, age:Int)
val alice=new Person("Alice",25)
val bob= new Person("Bob",32)
val charlie=new Person("Charlie",32)

for(person <- List(alice,bob,charlie)){
    person match{
        case Person("Alice",25) => println("Hi Alice!")
        case Person("Bob",32) => println("Hi Bob!")
        case Person(name,age)=> println(
        "Age: "+ age + " year, name: " + name + "?")
    }
}
```

- **Extractor로 패턴 매칭**
- Extractor는 패턴 매칭을 해야 하는 대상 값을 가져와서 필요한 값을 추출한 후 가공해서 내보낼 수 있다. 

```scala
object Emergency{
    def unapply(number: String):Boolean={
        if (number.length==3&&number.forall(_.isDigit)) true 
        else false
    }
}
object Normal{
    def unapply(number: String): Option[Int]={
        try{
            var o=number.replaceAll("-","")
            Some(number.replaceAll("-","").toInt)
        }catch{
            case _: Throwable => None
        }
    }
}
var number1="010-222-2222"
var number2="119"
var number3="포도 먹은 돼지"
var numberList=List(number1,number2,number3)
for(number <- numberList){
    number match{
        case Emergency() => println("긴급전화입니다.")
        case Normal(number) => println("일반전화입니다 - " + number)
        case _ => println("판단 할 수 없습니다.")
    }
}
```

- **컬렉션**
- ***배열*** 초기 값을 지정해서 배열을 선언하는 경우 자료형을 표시하지 않아도 알아서 자료형 할당

- ***리스트***  앞뒤가 연결된 리스트로서 내부적으로 리스트를 붙이거나 나누는데 효육적인 구조를 가지고 있다
- 동적으로 크기를 늘리거나 줄이는 것이 가능
- LIst는 추상 클래스 형태 혹은 이미 완성된 객체 형태로 존재하기 때문에 new를 사용하지 않는다
- 이미 만들어져있는 List정적 객체의 내부적인 팩토리 역활인 apply()가 동작하면서 새로운 List객체를 생성
- :: 는 리스트의 각 요소를 결합
- :::는 여러 리스트를 병합

```scala
val list1 ="a" :: "b" ::"c"::Nil
for(x <- 0 until list1.size)
println(list1(x))

val list2="d" ::"e"::Nil
val list0=list1 ::: list2
for(x <- 0 until list0.size){
println(list0(x))
}
```

- **Map**의 주요 기능
- 키를 통해 요소에 접근, 인덱스가 피룡하지 않다.

```scala
val list3="a"::"b"::"c"::Nil
val list4=1::2::3::Nil
val list5=2::2::4::Nil
println(list3 ++ list4)
println(list3.apply(0))
println(list3.reverse)
println(list4.max)
println(list4.min)
println(list4.sum)
println(list4.mkString(","))
println(list4.exists(a => 0>3))
println(list4.exists(_>3))
println(list4.contains(1))
println(list4.isEmpty)
println(list4.distinct)
```

```scala
val map=Map("number1" -> "aa",
            "number2" -> "bb",
           "number3" -> 3,
           5 -> "cc")
println(map.isEmpty) 
println("whole map:" +map)
println("keys:" +map.keys)
println("values: "+map.values)
println(map("number1"))
val map3= map + ("num4" -> 44)
println(map3)
val map2 =Map("n1"-> 100, "n2"-> 200)
val map4=map3 ++ map2
println(map4)
map4 - ("num4")
println(map4)
```

- **집합(set)**
- 중복되지 않는 값을 다뤄야 할때

```scala
var basket: Set[String] =Set()
basket+="딸기"
basket+="포도"
basket+="포도"
basket+="사과"
basket+="포도"
basket+="바나나"
println(basket)

var basket2: Set[String]=Set()
basket2 +="토마토"
basket2 +="당근"
basket2 +="감자"
basket2 +="사과"

println(basket.diff(basket2))
println(basket|baseket2)


```

- **튜플(tuple)**
- 여러 데이터를 하나의 묶음으로 처리하고 싶을 때 튜플로 처리
- 튜플은 N개의 데이터 쌍을 저장하는 자료 구조

```scala
val t1=(1,2) //튜플 생성
val t2=("a",1,"c")
val n1=t1._2//튜플 내용 참조 //2 결과값으로
val n2=t2._3// c 가 결과값으로
```

- **옵션**
- 여러 개의 값을 저장하는 자료 구조로서 값이 있을 수도 없을 수도 있는 경우에 사용
- None이거나 Some()을 하나 가지고 있다
- 맵에서 키를 이용해 값을 지겨울 떄, 해당하는 값이 있을 때는 Some()을 반환하고 없으면 None을 반환하여 로직에 사용
- 어떤 값이 들어 있으면 SOme 으로 래핑되어 있기 때문에  case Some()으로 패턴 매칭 할수 있고, 값이 없으면 None이기 떄문에 

```scala
object Ex{
    def main(args: Array[String]):Unit={
        val students=Map(
            1 -> "문진한",
            2 -> "엄다솔",
            3 -> "노순표"
           
        )
        val one=students.get(1)
        val four=students.get(4)
        
        println(one)
        println(four)
        println(one.get)
        println(four.getOrElse("값이 없습니다."))
    }
}
```

- **시퀀스**
- 내부적으로 인덱스에 대한 정보를 가지고 있고 인덱스와 관련해서 써야 할 기능이 많을 경우 쉽게 데이터를 다룰 수 있다.

```scala
val dounts: Seq[String] =Seq("Plain Dount", "Strawberry Dount","Glazed Dount")
println(s"Elements of dounts=$dounts")
println(s"Take elements from index 0 to 1=${dounts.slice(0,1)}")
println(s"Take elements from index 0 to 2=${dounts.slice(0,2)}")

```

- **이터레이터**
- hasNest와 next, length 등이 있다.

```scala
val list=List("a","b","c")
val i=list.iterator
while(i.hasNext)
println()
```

- **패턴매치**
- 스칼라에서는 패키지에 변수나 클래스 등을 선언 할 수 있다.
- 패키지 객체를 이용하면 Common과 같은 클래스를 정의하지 않고도 동일 패키지에서 사용하는 변수나 메서드 등을 공유 할 수 있다.
- package 키워드를 사용해 정의

```scala

```

- type은 새로운 타입을 선언하는 키워드
- 선언된 타입은 실제로 변수나 메서드의 타입으로 사용 가능
- 스칼라에서는 다른 클래스의 변수나 메서드 등을 사용하기 위해 import문을 사용
- 스칼라에서는 static키워드를 사용하지 않고, _를 사용해서 표기

- **함수 컴비네이터**
- 구현된 로직을 따라 컬렉션을 변형한 후 동일한 자료형의 컬렉션을 반환하는 역활을 맡는 메서드
- map(),foreach()컬렉션을 탐색하면서 그 안의 값을 바꾸는 역활
- map()리스트 자체를 변형하지 않고 List자료형을 반환 하면서 새로운 변수에 담게 한다
- foreach()아무값도 반환하지 않으며 리스트 자체의 값을 변형

```scala
val o=List(1,2,3,4)
println(o)
val n=o.map(i => i*10)
println(n)

val m=o.foreach(i => i*10)
```

- **filter(), filterNot()** -조건이 참, 거짓을 가릴 수 있는 형..?

```scala
val o =List(1,2,3,4)
println(o)
val n=o.filter(i => i => 3).map(i=>i*2)
println(n)
```

- **foldLeft(), foldRight()**컬렉션에 있는 여러 요소를 한쪽.

```scala
val o =List(1,2,3,4)
val sum=o.foldLeft(0.0)(_+_) 
println(s"Sum = $sum") //결과값은 10
```

- **partition()** 컬렉션을 나누는데 필요한 기능. 조건에 맞는 것들을 하나의 리스트로 저장하고 나머지 것을 다른 리스트에 저장

```scala
vol o=List(1,2,3,4)
val n=o.partition(i=> i<3)
println(n)
```

- **zip()** 두개의 리스트를 하나로 합친다.

```scala
val o=List(1,2,3,4)
val oo=List(5,6,7,8,9)
val n=o zip oo//<1,5), <2,6),(3,7),(4,8)
val nn=o:::oo//1,2,3,4,5,6,7,8,9
println(n)
println(nn)
```

- **find()** 원하는 조건에 맞는 첫 번째 요소를 반환

```scala
val o=List(1,2,3,4)
val n=o.find(i=> i>=2)
val nn=o.find(i=>i==9)
println(n)
println(nn)
```

- **drop(), dropWhile()**

```scala
val o=List(1,2,3,4,5,6,0)
val n=o.drop(4)//4 이하는 버린다.
val nn=o.dropWhile(i=> i<3)//조건식에 해당하는 것
println(n)
println(nn)
```

- **flattern()**리스트 안에 리스트가 중첩되어 있는 경우 풀어주는 기능을 수행

``` scala
val o=List(List(1,2,3,4),List(5,6))
val n=o.flatten
println(n)
```

```scala
val donuts1: Seq[String] =Seq("Plain","Strawberry","Glazed")
val donuts2: Seq[String]=Seq("Vanilla","Glazed")
val listDonuts: List[Seq[String]]=List(donuts1,donuts2)
val listDonutsFromFlatten:List[String]=listDonuts.flatten
```

- **예외 처리**
- try, catch, finally

```scala
import java.io.FileReader
import java.io.FileNotFoundException
import java.io.IOException
import java.io.Console

object Demo{
    def main(args: Array[String]): Unit={
        try{
            val f=new FileReader("input.txt")
            //읽는 것이 없기에 파일이 이써도 확인 못한다
            
        }catch{
            case ex: FileNotFoundException => {
                println("Missing file exception")
            }
            case ex: IOException => {
                println("IO Exception")
            }
        }finally{
            println("Exiting finally")
        }
    }
}
```

```scala
//파일 내용 읽기위해서
object Demo{
    def main(args: Array[String]){
        print("Please enter your input :")
        val line=Console.readLine
        println("Thanks, yout just typed: "+line)
    }
}
import scala.io.Source
object Demo{
    def main(args: Array[String]):Unit ={
        println("Following is the content read:")
        Source.fromFile("Demo.txt").foreach{
            print
        }
    }
}
```

```scala
//파일 내용 출력 하기 위해서
import scala.io.StdIn.readLine
import java.io.File
import java.io.PrintWriter


object Ex{
    def main(args: Array[String]):Unit ={
        val fileName="test.txt"
        var input=readLine("파일에 쓸 내용 입력 하세요!")
        
        val writer=new PrintWriter(new File(fileName))
        writer.write(input)
        writer.close
        //출력파일은 패키지 디렉토리에 생성된다.
        print( "입력하신 텍스트를 " + fileName+"에 저장했습니다.")
    }
}
```

- **either**
- 둘 중 하나를 선택

```scala
object Ex {   
   def main(args: Array[String]): Unit = {
       val input = scala.io.StdIn.readLine("값을 입력하세요:")
       val result: Either[String, Int] = try {
          Right(input.toInt)
       } catch {
         case e : Exception => Left(input)
       }
       print(result.getClass)             
   }
}

```





# 스파크

## 1. 정의

오픈소스 병렬분산 처리 플랫폼으로 스트림처리와 머신러닝을 효과적으로 수행하기 위함이다.

#### 1.1 스파크 구조

##### 1.1.1 복수의 컴포넌트로 구성 

1. sql처리
2. 스트림처리
3. 머신러닝
4. 그래프 처리(그래스X)

##### 1.1.2구성 요소

1. 클러스터 매니저 - Spark standalone, Yarn, Mesos
2. 코어 엔진- 스파크 코어
3. 라이브러리(스파크 sql(SQL처리), 스파크 스트리밍(스트림처리), MLlib(머신러닝) ,그래프X(그래프처리))

#####  1.1.3 스파크 자료구조

1. Spark에서의 데이터 처리하기 위한 추상화된 모델: RDD(복구가능한 분산 데이터셋)

2. SparkApplication => job

3. Spark 클러스터 환경에서의 node들 : SparkClient, Master 노드, Worker노드

4. SparkClient 역활

   > SparkApplication 배포하고 실행 요청

5. Spark Master 역활

   > Spark클러스터 환경에서 사용가능한 리소스들 관리

6. WorkNode 역활

   > 할당받은 리소스(CPU core,memory)를 사용해서 SparkApplication 실행
   >
   > Spark Worknode에서 실행되는 프로세스 -Executor는 RDD의 partition을 

##### 1.1.4 구현

SparkApplication 구현 단계

> 1. SparkContext 생성
>
>    SparkContext 은 스파크 진입점, 실행환경 클라이언트
>
> 2. RDD 생성
>
>    collection, HDFS, hive, CSV 등 으로 생성 가능 
>
>    불변데이터 모델, partition가능
>
> 3. RDD 처리
>
>    변환연산(RDD의 요소의 구조 변경, filter처리, grouping,...) RDD를 인수로 받아 새로운 RDD를 생성. **불변, 지연**특성을 가짐
>
> 4. 집계요약처리
>
>    Action연산(가공하지 않는 활동)
>
> 5. 영속화
>
>    파일로 저장 할 수 있다.

##### 1.1.5 장점

1. 메모리 기반임으로 반복처리와 연속으로 이루어지는 변환처리를 고속화 가능
2. 딥러닝,머신러닝들의 실행환경에 적합한 환경 제공
3. 서로 다른 실행환경과 구조, 데이터들의 처리에 대해서 통합 환경 제공

##### 1.1.6 메서드

- **sc.textFile()** :file로 부터 RDD생성
- **collect **배열만들기
- **Map , flatMap()**
- **mkString("구분자")**   RDD요소 출력.  배열의 요소를 문자열과 결합

***

## 설치하기

1. https://www.apache.org/dyn/closer.lua/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz 에서

   http://apache.tt.co.kr/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz 클릭해서 다운

```cmd
su -
cd usr/local
tar -zxvf /home/hadoop/Downloads/spark-2.4.3-bin-hadoop2.7.tgz
ls -l

ln -s spark-2.4.3-bin-hadoofp2.7 spark #심볼릭링크 만들기
ls -l
chown -R hadoop:hadoop spark
ls -l
```

 2. 

```cmd
su hadoop
vi .bash_profile
#설정 저장 위치는 
export SPARK_HOME=/usr/local/spark
export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
# export PATH=$PATH:$JAVA_HOME 에 추가해야한다
/bin:$SPARK_HOME/          # 애는 잠시 지운다. bin:$SBT_HOME/bin
PATH=$PATH:$HOME/.local/bin:$HOME/bin

export JAVA_HOME=/usr/local/jdk1.8.0_221
export HADOOP_HOME=/usr/local/hadoop-2.7.7
export HADOOP_CMD=/usr/local/hadoop-2.7.7/bin/hadoop
export HADOOP_STREAMING=/usr/local/hadoop-2.7.7/share/hadoop/tools/lib/hadoop-streaming-2.7.7.jar
export HIVE_HOME=/usr/local/hive
export SPARK_HOME=/usr/local/spark
export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin:
#path가 밑에잇어야 한다. 그러므로 path 위에 export를 추가하자
source .bash_profile
```



#### 클러스터 구성의 스파크 동작 확인

리눅스의 하둡계정을 로그아웃했다가 로그온 해주자~

```cmd
#hadoop 계정에서
spark-shell --master local verbose
scala> #로 들어가진다.
```



#### 실행해보자

```scala
//로컬 파일시스템에서 파일을 읽어들여서 RDD로 생성
scala> val file=sc.textFile("file:///usr/local/spark/README.md")
//RDD로부터 한 행(라인)단위로 처리-단어 분리 후 RDD 저장 생성
scala> val words=file.flatMap(_.split(" "))
//여기까지가 변환처리다

//같은 단어끼리 모아서 요약(개수) 계산-map형태로 단어와 출현횟수
scala> val result=words.countByValue

scala> result.get("For").get

```



## sbt 설치하기

스파크 애플리케이션은 소스코드를 컴파일 하고 JAR파일로 패키징해야 하는데 이를 해주는 것이 sbt이다. sbt는 스칼라와 자바로 기술된 소스코드를 통합 관리하기 위한 툴이다.(패키지 작성, 애플리케이션 개발 프로젝트 빌드 프로세스 관리 등 )

https://www.scala-sbt.org/download.html  sbt 1.2.7.tgz를 받자

```cmd
#root 계정(su -)
[root@master ~]# tar -zxvf /home/hadoop/Downloads/sbt-1.2.7.tgz -C /opt/
[root@master ~]# ls -l /opt
# 소유자가 hadoop이라 안 바꿔도 된다.


[hadoop@master ~]$ vi .bash_profile

export SBT_HOME=/opt/sbt
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin:$SBT_HOME/bin
[hadoop@master ~]$ source .bash_profile
#로그아웃 했다가 로그온!

[hadoop@master ~]$ sbt about
#로 정보 확인
```



#### sbt로 스파크 애플리케이션 작성

1. sbt 프로젝트 디렉토리 구조 만들기

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir  spark-simple-app

#소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ cd spark-simple-app/
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala

#sbt 설정 파일 저장 디렉토리
[hadoop@master spark-simple-app]$ mkdir project

#소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master spark-simple-app]$ cd src/main/scala/lab/spark/example/
# 뒤의 새게의 파일이 바로 package명이다!
[hadoop@master example]$ vi SundayCount.scala

```

#### SundayCount.scala 작성

```javascript
package lab.spark.example

import org.joda.time.{DateTime,DateTimeConstants}
import org.joda.time.format.DateTimeFormat
import org.apache.spark.{SparkConf,SparkContext}

object SundayCount{
def main(args: Array[String]){
if (args.length<1){
throw new IllegalArgumentException(
"명령 인구 경로")
}

val filePath=args(0)
val conf=new SparkConf
val sc=new SparkContext(conf)

try{
val textRDD=sc.textFile(filePath)
val dateTimeRDD=textRDD.map{dateStr=>
val pattern=
DateTimeFormat.forPattern("yyyyMMdd")
DateTime.parse(dateStr,pattern)ㅗㅝㅓ
}
val sundayRDD=dateTimeRDD.filter{dateTime=>
dateTime.getDayOfWeek==DateTimeConstants.SUNDAY
}
val numOfSunday=sundayRDD.count
println(s"주어진 데이터에는 일요일이${numOfSunday}개 들어 있습니다.")
}finally{
sc.stop()
}
}
}

```

#### build.sbt 작성



```cmd
[hadoop@master ~]$ cd ~/spark-simple-app
[hadoop@master spark-simple-app]$ vi build.sbt

name :="spark-simple-app"
version :="0.1"
scalaVersion:="2.11.12"
libraryDependencies ++=
Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided",
"joda-time" % "joda-time" %"2.8.2")
assemblyOption in assembly :=(assemblyOption in assembly).value.copy(includeScala=false)

```



#### plugins.sbt작성과 sbt-assembly이용에 필요한 설정

```cmd
[hadoop@master spark-simple-app]$ cd project
[hadoop@master project]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n"%"sbt-assembly"%"0.14.10")
# 저장하고 나오자~
[hadoop@master project]$ cd ~/spark-simple-app/
[hadoop@master spark-simple-app]$ sbt assembly

```

Packaging /home/hadoop/spark-simple-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar  여기에 jar가 생겼다.

jar로 패키지 한 것!

#### jar파일 실행해보자. 

1. 먼저 date.txt를 만들고 하둡에 올린다!

```cmd
[hadoop@master spark-simple-app]$ cd ~
[hadoop@master ~]$ vi date.txt

20150322
20150331
20150417
20150426
20150506
20150523
20150524
20150712
20150727
20150810
20150830
20150927

# 작성 (공백없이 작성해달라)


[hadoop@master ~]$ hadoop fs -mkdir /data/spark/
[hadoop@master ~]$ hadoop fs -put date.txt /data/spark
[hadoop@master ~]$ hadoop fs -ls /data/spark
Found 1 items
-rw-r--r--   1 hadoop supergroup        116 2019-08-27 13:37 /data/spark/date.txt
# 확인하자


[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.SundayCount --name SundayCount ~/spark-simple-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/spark/date.txt

#"" 오류가 생겼다면 date.txt의 공백을 다 지운상태에서 하둡의 파일을 지우고 다시 -put 하자~ 그리고 다시 assembly
```



### 스파크 WordCount 예제

1. SparkContext생성
2. (입력 소스로부터) RDD 생성
3. RDD 처리
4. 결과 파일 처리
5. SparkContext종료

```scala

```



## 스파크 셀

1. Spark Context
   - 애플리케이션과 스파크 클러스터와의 연결을 담당
   - 모든 스파크 애플리케이션은 SparkContext를 이용하여 RDD나 accumulator또는 broadcase변수 등을 다룬다.
   - 스파크 애플리케이션을 수행하는 데 필요한 각종 설정 정보를 담는다.
2. RDD 생성
3. 스파크 클러스터
4. 분산 데이터로서 RDD
5. 파티션

### SparkContext

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir wordcount-app

[hadoop@master ~]$ cd wordcount-app

# 소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala  
#sbt 설정 파일 저장  디렉토리 생성
[hadoop@master ~]$ mkdir project

# 소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master ~]$ cd  src/main/scala/lab/spark/example
[hadoop@master ~]$ vi WordCount.scala
#---------------------------------------------------------------------------------------------
package lab.spark.example

import org.apache.spark.{SparkConf,SparkContext}

object WordCount{
def main(args: Array[String]){
require (args.length>=1,
"드라이버 프로그램의 인수에 단어를 세고자 하는"+
"파일의 경로를 지정해 주세요.")

val conf=new SparkConf
val sc =new SparkContext(conf)

try{
val filePath=args(0)
val wordAndCountRDD = sc.textFile(filePath)
.flatMap(_.split("[ ,.]"))
.filter(_.matches("""\p{Alnum}+"""))
.map((_, 1))
.reduceByKey(_+_)

wordAndCountRDD.collect.foreach(println)
}finally{
sc.stop()
}
}
}

#---------------------------------------------------------------------------------------------
[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ vi build.sbt

name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)


[hadoop@master ~]$ cd project
[hadoop@master ~]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")


#어플리케이션 빌드
[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ sbt assembly

#데이터 소스 생성
[hadoop@master ~]$ vi simple-words.txt
cat
dog
.org
cat
rabbit
bear
cat
&&
tiger
dog
rabbit
100
bear
tiger
cat
rabbit
?bear

#하둡 파일 시스템에 simple-words.txt파일 업로드
[hadoop@master ~]$ hadoop fs -mkdir  /data/wordcount
[hadoop@master ~]$ hadoop fs -put simple-words.txt  /data/wordcount


[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.WordCount --name WordCount ~/wordcount-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/wordcount


```

#### 스파크 RDD 

- **cogroup**
- RDD의 구성요소가 키와 값의 쌍으로 이루어진 경우 사용 가능

```scala
var rdd=sc.parallelize(List(("k1","v1"),("k2","v2"),("k1","v3")))
var rdd2=sc.parallelize(List("k1","v4"))
var result=rdd.cogroup(rdd2)
result.collect.foreach{
    case(k, (v_1,v_2))=> {
        println(s"($k,[${v_1.mkString(",")}],[$(v_2.mkString(", "))])")
    }
}
```



- **distinct()**
- RDD원소에서 중복을 제외한 요소로만 구성

```scala
var rdd=sc.parallelize(List(1,2,3,1,2,3,1,2,3))
var result=rdd.
```



- **substract()**
- 차집합

```scala
val rdd1 = sc.parallelize( List("a", "b", "c", "d", "e"))
val rdd2 = sc.parallelize( List("d", "e"))
val result = rdd1.substract(rdd2)
println(result.collect.mkString(", "))

```

- **intersecton()**
- 두개의 RDD가 있을 때 rdd1과 rdd2에 동시에 속하는 요소로 구성된 새로운 RDD를 생성하는 메서드
- 결과로 생성되는 RDD에는 중복된 원소가 존재하지 않는다.

```scala
val rdd1 = sc.parallelize( List( "a", "a", "b", "c"))
val rdd2 = sc.parallelize( List( "a", "a", "c", "c"))
val result = rdd1.intersection(rdd2)
println(result.collect.mkString(", "))

```

- **leftOuterJoin(), rightOuterJoin()**
- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용 가능
- 조인 결과를 표시할 떄는 값이 존재하지 않는 경우를 고려해 Option타입을 이용

```scala
val rdd1 = sc.parallelize( List( "a", "a", "b", "c")).map((_, 1))
val rdd2 = sc.parallelize( List( "b", "c")).map((_, 2))
val result1 = rdd1.leftOuterJoin(rdd2)
val result2 = rdd1.rightOuterJoin(rdd2)
println("Left:" + result1.collect.mkString("\t"))
println("Right:" + result2.collect.mkString("\t"))

```

- **substractByKey()**
- rdd1의 요소 중 rdd2에 같은 키가 존재하는 요소를 제외한 나머지로 구성된 새로운 RDD

```scala
val rdd1 = sc.parallelize( List("a", "b")).map(_, 1))
val rdd2 = sc.parallelize( List("b" )).map((_, 2))
val result = rdd1. substractByKey(rdd2)
println(result.collect.mkString("\n"))

```

- **reduceByKey()**
- RDD의 구성요소가 키와 값으로 구성된 경우에 사용 가능
- 같은 키를 가진 값들을 하나로 병합하여 키-값 쌍으로 구성된 새로운 RDD생성

```scala
var rdd = sc.parallelize( List( "a", "b", "b")).map((_, 1))
var result = rdd.reduceByKey(rdd)
println(result.collect.mkString(","))

```

- **foldByKey()**

```scala
val rdd = sc.parallelize( List( "a", "b", "b")).map((_, 1))
val result = rdd.foldByKey(_+_)
println(result.collect.mkString(","))

```

- **combineBykey()**
- 값을 병합하기 위해 사용
- 파티션 단위로 사용 가능

```scala
def  reduceByKey(func: (V, V) => V) : RDD[(K, V)]
def  foldByKey(zeroValue: V)(func: (V, V) => V) : RDD[(K, V)] 
combineByKey[C](createCombiner:(V)=>C, mergeValue:(C, V)=>C, mergeCombiners: (C, C) => C):RDD[(K, C)]

```



```scala
//콤바이너 역활을 할 Record클래스 정의
case class Record(var amount: Long, var number: Long=1){//메서드 파라미터에서 기본값 설정
    def map(v:Long)=Record(v)
    def add(amount:Long):Record={
        add(map(amount))
    }
    def add(other:Record):Record={
        this.number+=other.number
        this.amount += other.amount
        this
    }
    override def toString =s"avg:${amount/number}"
}
//combineByKey()를 이용 한 평균값 계산
val data =Seq(("Math",100L),("Eng",80L),("Math",50L),("Eng",60L),("Eng",90L))
val rdd=sc.parallelize(data)
val createCombiner=(v:Long)=>Record(v)
val mergeValue =(c:Record,v:Long)=>c.add(v)
val mergeCombiners=(c1:Record,c2:Record)=>c1.add(c2)
val result=rdd.combineByKey(createCombiner,mergeValue,mergeCombiners)
println(result.collect.mkString("\n"))

(Math,avg:75)
(Eng,avg:76)


```

#### 집계와 관련된 연산

- **aggregateByKey()**

- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우 사용 가능

- 병합 시작할 초기값 생성, combineByKey()와 동일한 동작 수행

- **pipe()**

- Pipe를 이용하면 데이터를 처리 도중 외부 프로세스 활용 가능

- **repartitionAndSortWithinPartitions()**

- ```scala
  import org.apache.spark.HashPartitioner
  
  val r = scala.util.Random
  val data = for (i <- 1 to 10) yield (r.nextInt(100), "-")//난수 생성
  val rdd1 = sc.parallelize(data)
  val rdd2 = rdd1.repartitionAndSortWithinPartitions(new HashPartitioner(3))//hash방식은 파티션을 균등하게 가지는 것, 3개로 분할이 될 거다
  //결과 검증
  rdd2.foreachPartition(it => { println("========"); it.foreach(v=>println(v)) })//파티션 단위로 반복문을 통해 출력
  
  ```



#### 데이터 순서 바꾸어 처리하기

```scala
scala>val textRDD=sc.textFile("/data/spark/README.md")
//"file:///usr/local/spark/README.md" 여기에 있는 파일을 하둡의 위에 위치에 저장해놓자.

scala>val wordCandidateRDD=textRDD.flatMap(_.split("[ ,.]"))
scala>val wordRDD=wordCandidateRDD.filter(_.matches("""\p{Alnum}+"""))
scala>val wordAndOnePairRDD=wordRDD.map((_, 1))
scala>val wordAndCountRDD=wordAndOnePairRDD.reduceByKey(_+_)

scala> val countAndWordRDD=wordAndCountRDD.map{wordAndCount => (wordAndCount._2, wordAndCount._1)}

scala> val sortedCWRDD=countAndWordRDD.sortByKey(false)
scala> val sortedWCRDD=sortedCWRDD.map{countAndWord=> (countAndWord._2,countAndWord._1)}
scala> val sortedWCArray=sortedWCRDD.collect
scala> sortedWCArray.foreach(println)

```



#### RDD 선두로부터 요소 꺼내기

```
// top 3만 뽑기
val top3WordArray=sortedWCRDD.take(3)
top3WordArray.foreach(println)
```

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir wordcount-top3

[hadoop@master ~]$ cd wordcount-top3

# 소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala  
#sbt 설정 파일 저장  디렉토리 생성
[hadoop@master ~]$ mkdir project

# 소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master ~]$ cd  src/main/scala/lab/spark/example
[hadoop@master ~]$ vi WordCountTop3.scala
```

```java
package lab.spark.example

import org.apache.spark.{SparkConf, SparkContext}

object WordCountTop3{
    def main(args: Array[String]){
        require(args.length>=1,
               "드라이버 프로그램의 인수에 단어를 세고자 하는 "+
               "파일의 경로를 지정해 주세요.")
            val conf=new SparkConf
            val sc=new SparkContext(conf)
            try{
                //모든 단어에 대해(단어, 등장횟수)형의 튜플을 만든다.
                val filePath=args(0)
                val wordAndCountRDD =sc.textFile(filePath)
                    .flatMap(_.split("[ ,.]"))
                    .filter(_.matches("""\p{Alnum}+"""))
                    .map((_,1))
                    .reduceByKey(_+_)
                    //등장 횟수가 가장 많은 단어 세 개를 찾는다.
                    val top3Words=wordAndCountRDD.map{
                    case(word,count)=>(count,word)}.sortByKey(false).map{
                    case(count,word)=>(word,count)}.take(3)
                    //등장 획수가 가장 많은 단어 top3를 표준 출력으로 표시
                    top3Words.foreach(println)
                }finally{
                sc.stop()
            }
                }
            }
 

```

```cmd
[hadoop@master ~]$ cd ~/wordcount-top3
[hadoop@master ~]$ vi build.sbt

name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)


[hadoop@master ~]$ cd project
[hadoop@master ~]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")


#어플리케이션 빌드
[hadoop@master ~]$ cd ~/wordcount-top3
[hadoop@master ~]$ sbt assembly
[hadoop@master ~]$ cd ~
[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.WordCountTop3 --name WordCountTop3 ~/wordcount-top3/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/wordcount

(rabbit,3)
(cat,3)
(tiger,2)
 
#로 결과값 나온다.

```

#### 복수의 데이터를 결합해 처리

```csv
products.csv==============================
0,송편(6개),12000
1,가래떡(3개),16000
2,연양갱,5000
3,호박엿(6개),16000
4,전병(20장),4000
5,별사탕,3200
6,백설기,3500
7,약과(5개),8300
8,강정(10개),15000
9,시루떡,6500
10,무지개떡,4300
11,깨강정(5개),14000
12,수정과(6컵),19000
13,절편(10개),15000
14,팥떡(8개),20000
15,생과자(10개),17000
16,식혜(2캔),21000
17,약식,4000
18,수수팥떡(6개),28000
19,팥죽(4개),16000
20,인절미(4개),10000
 
  

sales-october.csv===============================

5830,2014-10-02 10:20:38,16,28
5831,2014-10-02 15:13:04,15,22
5832,2014-10-02 15:21:53,2,10
5833,2014-10-02 16:22:05,18,13
5834,2014-10-06 12:04:28,19,18
5835,2014-10-06 12:54:13,10,18
5836,2014-10-06 15:43:54,1,8
5837,2014-10-06 17:33:19,10,22
5838,2014-10-11 10:28:00,20,19
5839,2014-10-11 15:00:32,15,3
5840,2014-10-11 15:06:04,15,14
5841,2014-10-11 15:45:38,18,1
5842,2014-10-11 16:12:56,4,5
5843,2014-10-13 10:13:53,3,12
5844,2014-10-13 15:02:23,15,19
5845,2014-10-13 15:12:08,6,6
5846,2014-10-13 17:17:20,10,9
5847,2014-10-18 11:08:11,15,22
5848,2014-10-18 12:01:47,3,8
5849,2014-10-18 14:25:25,6,10
5850,2014-10-18 15:18:50,10,16
5851,2014-10-20 13:06:00,11,21
5852,2014-10-20 16:07:04,13,29
5853,2014-10-20 17:29:24,5,4
5854,2014-10-20 17:47:39,8,17
5855,2014-10-23 10:02:10,2,24
5836,2014-10-23 11:22:53,8,19
5857,2014-10-23 12:29:16,7,7
5858,2014-10-23 14:01:56,12,26
5859,2014-10-23 16:09:39,8,13
5860,2014-10-23 17:26:46,8,19

sales-november.csv====================================
5861,2014-11-01 10:47:52,15,22
5863,2014-11-01 11:44:54,8,26
5864,2014-11-01 14:29:51,18,10
5865,2014-11-01 17:50:00,6,17
5867,2014-11-04 10:03:57,15,16
5868,2014-11-04 11:22:55,15,13
5869,2014-11-04 16:32:09,19,6
5870,2014-11-10 11:12:30,17,27
5871,2014-11-10 13:32:53,17,13
5872,2014-11-10 15:31:21,4,15
5873,2014-11-10 16:03:01,6,5
5874,2014-11-10 17:52:20,12,28
5875,2014-11-15 11:36:39,3,5
5876,2014-11-15 14:08:26,9,7
5877,2014-11-15 15:05:21,10,0
5878,2014-11-18 11:17:09,7,16
5879,2014-11-18 14:50:37,9,3
5880,2014-11-18 16:23:39,4,20
5881,2014-11-18 17:28:31,18,25
5882,2014-11-22 10:50:24,7,26
5883,2014-11-22 11:43:31,3,3
5884,2014-11-22 12:57:22,4,12
5885,2014-11-22 15:20:17,19,25
5886,2014-11-25 16:42:07,10,27
5887,2014-11-25 17:38:03,14,0
5888,2014-11-25 18:30:36,10,8
5889,2014-11-25 18:41:57,11,10
5890,2014-11-30 14:30:08,11,17
5862,2014-11-30 14:57:47,8,22
5866,2014-11-30 15:17:29,8,24


```

```cmd
[hadoop@master ~]$ vi products.csv
[hadoop@master ~]$ vi sales-october.csv
[hadoop@master ~]$ vi sales-november.csv
#각각 저장 후 put 해주자.
[hadoop@master ~]$ hadoop fs -mkdir /data/csv
[hadoop@master ~]$ hadoop fs -put /home/hadoop/sales-november.csv  /data/csv
[hadoop@master ~]$ hadoop fs -put /home/hadoop/sales-october.csv  /data/csv 
 [hadoop@master ~]$ hadoop fs -put /home/hadoop/products.csv  /data/csv

```

```scala
def createSalesRDD(csvFile:String)={
    val logRDD =sc.textFile(csvFile)
    logRDD.map{
        record=>
        val splitRecord=record.split(",")
        val productId=splitRecord(2)
        val numOfSold=splitRecord(3).toInt
        (productId,numOfSold)
    }
}

val salesOctRDD=createSalesRDD("/data/csv/sales-october.csv")
val salesNovRDD=createSalesRDD("/data/csv/sales-november.csv")

import org.apache.spark.rdd.RDD
def createOver50SoldRDD(rdd:RDD[(String,Int)])={
    rdd.reduceByKey(_+_).filter(_._2 >= 50)
}
val octOver50SoldRDD=createOver50SoldRDD(salesOctRDD)
val novOver50SoldRDD=createOver50SoldRDD(salesNovRDD)

val bothOver50SoldRDD=octOver50SoldRDD.join(novOver50SoldRDD)
bothOver50SoldRDD.collect.foreach(println)

val over50SoldAndAmountRDD=bothOver50SoldRDD.map{
    case(productId,(octAmount,novAmount))=>
    (productId,octAmount+novAmount)
}
over50SoldAndAmountRDD.collect.foreach(println)
(8,(68,72))
(15,(80,51))

```

#### 브로드캐스트 변수

```scala
import scala.collection.mutable.HashMap
import java.io.{BufferedReader,InputStreamReader}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem,Path}
val productsMap =new HashMap[String,(String,Int)]
val hadoopConf=new Configuration
val fileSystem=FileSystem.get(hadoopConf)
val inputStream=fileSystem.open(new Path("/data/csv/products.csv"))
val productsCSVReader=new BufferedReader(new InputStreamReader(inputStream))
var line=productsCSVReader.readLine
while(line!=null){
    val splitLine=line.split(",")
    val productId=splitLine(0)
    val productName=splitLine(1)
    val unitPrice=splitLine(2).toInt
    productsMap(productId)=(productName,unitPrice)
    line = productsCSVReader.readLine
}
productsCSVReader.close()

val broadcasetedMap=sc.broadcast(productsMap)

val resultRDD=over50SoldAndAmountRDD.map{
    case(productId,amount)=>
    val productsMap=broadcasetedMap.value
    val(productName,unitPrice)=productsMap(productId)
    (productName,amount,amount * unitPrice)
}
resultRDD.collect.foreach(println)

(강정(10개),140,2100000)
(생과자(10개),131,2227000)

```



spark-submit --master local --class lab.spark.example.QuestionnaireSummarization --name Questionnaire ~/Questionnaire/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/scv/

#### questionnaireRDD  집계처리

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir 

[hadoop@master ~]$ cd Questionnaire

# 소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala  
#sbt 설정 파일 저장  디렉토리 생성
[hadoop@master ~]$ mkdir project

# 소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master ~]$ cd  src/main/scala/lab/spark/example
[hadoop@master ~]$ vi QuestionnaireSummarization.scala
#-----------------------------------------
```



```java
package lab.spark.example

import org.apache.spark.{Accumulator, SparkConf, SparkContext}
import org.apache.spark.rdd.RDD

object QuestionnaireSummarization {

  /**
   * 모든 앙케이트의 평가 평균값을 계산하는 메소드
   */ 
  private def computeAllAvg(rdd: RDD[(Int, String, Int)]) = {
    val (totalPoint, count) =
      rdd.map(record => (record._3, 1)).reduce {
        case ((intermedPoint, intermedCount), (point, one)) =>
          (intermedPoint + point, intermedCount + one)
      }
    totalPoint / count.toDouble
  }

  /**
   * 연령대별 평강의 평균값을 계산하는 메소드
   */
  private def computeAgeRangeAvg(rdd: RDD[(Int, String, Int)]) = {
    rdd.map(record => (record._1, (record._3, 1))).reduceByKey {
      case ((intermedPoint, intermedCount), (point, one)) =>
        (intermedPoint + point, intermedCount + one)
    }.map {
      case (ageRange, (totalPoint, count)) =>
        (ageRange, totalPoint / count.toDouble)
    }.collect
  }

  /**
   * 남녀별 평가의 평균값을 계산하는 메소드
   */
  private def computeMorFAvg(
      rdd: RDD[(Int, String, Int)],
      numMAcc: Accumulator[Int],
      totalPointMAcc: Accumulator[Int],
      numFAcc: Accumulator[Int],
      totalPointFAcc: Accumulator[Int]) = {
    rdd.foreach {
      case (_, maleOrFemale, point) =>
        maleOrFemale match {
          case "M" =>
            numMAcc += 1
            totalPointMAcc += point
          case "F" =>
            numFAcc += 1
            totalPointFAcc += point
        }
    }
    Seq(("Male", totalPointMAcc.value / numMAcc.value.toDouble),
      ("Female", totalPointFAcc.value / numFAcc.value.toDouble))
  }

  def main(args: Array[String]) {
    require(
      args.length >= 2,
       """
        |애플리케이션의 인자에
        |<앙케이트 CSV 파일의 경로>
        |<출력하는 결과파일의 경로>를 지정해 주세요. """.stripMargin)

    val conf = new SparkConf
    val sc = new SparkContext(conf)

    try {
      val filePath = args(0)

      // 앙케이트를 로드해 (연령대, 성별, 평가) 형식의
      // 튜플을 요소로 하는 RDD를 생성한다.
      val questionnaireRDD = sc.textFile(filePath).map { record =>
        val splitRecord = record.split(",")
        val ageRange = splitRecord(0).toInt / 10 * 10
        val maleOrFemale = splitRecord(1)
        val point = splitRecord(2).toInt
        (ageRange, maleOrFemale, point)
      }

      // questionnaireRDD는 각각의 집계처리에 이용되기 때문에 캐시에 보존한다
      questionnaireRDD.cache

      // 모든 평가의 평균치를 계산한다
      val avgAll = computeAllAvg(questionnaireRDD)
      // 연령대별 평균치를 계산한다
      val avgAgeRange = computeAgeRangeAvg(questionnaireRDD)

      // 성별이 M인 앙케이트의 건수를 세는 어큐뮬레이터
      val numMAcc = sc.accumulator(0, "Number of M")
      // 성별이 M인 앙케이트의 평가를 합계하는 어큐뮬레이터
      val totalPointMAcc = sc.accumulator(0, "Total Point of M")
      // 성별이 F인 앙케이트의 건수를 세는 어큐뮬레이터
      val numFAcc = sc.accumulator(0, "Number of F")
      // 성별이 F인 앙케이트의 평가를 합계하는 어큐뮬레이터
      val totalPointFAcc = sc.accumulator(0, "TotalPoint of F")

      // 남여별 평균치를 계산한다
      val avgMorF = computeMorFAvg(
        questionnaireRDD,
        numMAcc,
        totalPointMAcc,
        numFAcc,
        totalPointFAcc)

      println(s"AVG ALL: $avgAll")
      avgAgeRange.foreach {
        case (ageRange, avg) =>
          println(s"AVG Age Range($ageRange): $avg")
      }

      avgMorF.foreach {
        case (mOrF, avg) =>
          println(s"AVG $mOrF: $avg")
      }
    } finally {
      sc.stop()
    }
  }
}
```

```cmd


[hadoop@master ~]$ cd ~/Questionnaire
[hadoop@master ~]$ vi build.sbt

name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)


[hadoop@master ~]$ cd project
[hadoop@master ~]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")
# csv 파일 저장
[hadoop@master ~]$ vi questionnaire.csv
23,F,3
22,F,5
20,M,4
35,F,2
33,F,4
18,M,4
28,M,5
42,M,3
18,M,3
56,F,2
53,M,1
30,F,4
19,F,5
17,F,4
33,M,4
26,F,3
22,F,2
27,M,4
45,F,2
[hadoop@master ~]$ hadoop fs -put /home/hadoop/questionnaire.csv  /data/csv  
# 하둡에 저장


#어플리케이션 빌드
[hadoop@master ~]$ cd ~/Questionnaire
[hadoop@master ~]$ sbt assembly
[hadoop@master ~]$ cd ~
[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.QuestionnaireSummarization --name Questionnaire ~/Questionnaire/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/csv/questionnaire.csv/ /oooooo
# 코드안에 경로에 저장하는 코드가 없기 때문에 저장은 안된다~
```



## 스파크 SQL



```scala
//RDD로부터 DataFrame생성
case class Dessert(menuId:String, name:String, price:Int,kcal:Int)

val dessertRDD=sc.textFile("/data/DataFrame/dessert-menu.csv")
val dessertDF=dessertRDD.map{record=>
     val splitRecord=record.split(",")
     val menuId=splitRecord(0)
      val name =splitRecord(1)
    val price=splitRecord(2).toInt
   val kcal=splitRecord(3).toInt
     Dessert(menuId,name,price,kcal)}.toDF

dessertDF.printSchema

//DataFrame으로부터 RDD생성
var rowRDD=dessertDF.rdd
val nameAndPriceRDD=rowRDD.map{row=>
     val name=row.getString(1)
     val price=row.getInt(2)
     (name,price)}

scala> nameAndPriceRDD.collect.foreach(println)
(초콜릿 파르페,4900)
(푸딩 파르페,5300)
(딸기 파르페,5200)
(판나코타,4200)
(치즈 무스,5800)
(아포가토,3000)
(티라미스,6000)
(녹차 파르페,4500)
(바닐라 젤라또,3600)
(카라멜 팬케익,3900)
(크림 안미츠,5000)
(고구마 파르페,6500)
(녹차 빙수,3800)
(초코 크레이프,3700)
(바나나 크레이프,3300)
(커스터드 푸딩,2000)
(초코 토르테,3300)
(치즈 수플레,2200)
(호박 타르트,3400)
(캬라멜 롤,3700)
(치즈 케익,4000)
(애플 파이,4400)
(몽블랑,4700)

//DataFrame에 쿼리 발행
scala> dessertDF.registerTempTable("dessert_table")
scala> val numOver300KcalDF=spark.sqlContext.sql("SELECT count(*) AS num_of_over_300Kcal From dessert_table WHERE kcal >= 300")
scala> numOver300KcalDF.show
+-------------------+
|num_of_over_300Kcal|
+-------------------+
|                  9|
+-------------------+
//스파크 SQL 내장 함수의 사용 예
scala> spark.sqlContext.sql("SELECT atan2(1, 3)" ).show
+-------------------------------------------+
|ATAN2(CAST(1 AS DOUBLE), CAST(3 AS DOUBLE))|
+-------------------------------------------+
|                         0.3217505543966422|
+-------------------------------------------+
//하이브 내장 함수의 사용 예
scala> spark.sqlContext.sql("SELECT pi() AS PI, e() AS E").show
+-----------------+-----------------+
|               PI|                E|
+-----------------+-----------------+
|3.141592653589793|2.718281828459045|
+-----------------+-----------------+




```



- 스파크 SQL의 내장 함수

  https://gool.gl/ukXtJH

- 하이브의 내장 함수

  https://goo.gl/Vda7h

```scala
val nameAndPriceDF=dessertDF.select(dessertDF("name"),dessertDF("price"))
nameAndPriceDF.printSchema

root
 |-- name: string (nullable = true)
 |-- price: integer (nullable = false)



val selectAllDF=dessertDF.select("*")
selectAllDF.printSchema
root
 |-- menuId: string (nullable = true)
 |-- name: string (nullable = true)
 |-- price: integer (nullable = false)
 |-- kcal: integer (nullable = false)


nameAndPriceDF.show

+---------------+-----+
|           name|price|
+---------------+-----+
|  초콜릿 파르페| 4900|
|    푸딩 파르페| 5300|
|    딸기 파르페| 5200|
|       판나코타| 4200|
|      치즈 무스| 5800|
|       아포가토| 3000|
|       티라미스| 6000|
|    녹차 파르페| 4500|
|  바닐라 젤라또| 3600|
|  카라멜 팬케익| 3900|
|    크림 안미츠| 5000|
|  고구마 파르페| 6500|
|      녹차 빙수| 3800|
|  초코 크레이프| 3700|
|바나나 크레이프| 3300|
|  커스터드 푸딩| 2000|
|    초코 토르테| 3300|
|    치즈 수플레| 2200|
|    호박 타르트| 3400|
|      캬라멜 롤| 3700|
+---------------+-----+
only showing top 20 rows


selectAllDF.show(3)
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|   D-0|초콜릿 파르페| 4900| 420|
|   D-1|  푸딩 파르페| 5300| 380|
|   D-2|  딸기 파르페| 5200| 320|
+------+-------------+-----+----+
only showing top 3 rows

```

- 1000원의 가격을 달러로 표시

```scala
val nameAndDollarDF=nameAndPriceDF.select($"name",$"price"/lit(1000.0))
nameAndDollarDF.printSchema

val nameAndDollarDF=nameAndPriceDF.select(
$"name",($"price"/lit(1000.0)) as "dollar price")

nameAndDollarDF.printSchema

//필터링(where())
val over5200WonDF=dessertDF.where($"price" >=5200)
over5200WonDF.show

val over5200WonNameDF=dessertDF.where($"price" >=5200).select($"name")
over5200WonNameDF.show
//정렬(orderBy())
val sortedDessertDF=dessertDF.orderBy($"price".asc,$"kcal".desc)
sortedDessertDF.show
//집약처리(agg())
val avgKcalDF=dessertDF.agg(avg($"kcal") as "avg_of_kcal")
avgKcalDF.show

import org.apache.spark.sql.types.DataTypes._
val numPerPriceRangeDF=dessertDF.groupBy(
(($"price"/1000) cast IntegerType)*1000 as "price_range").agg(count($"price")).orderBy($"price_range")
numPerPriceRangeDF.show
```

- DataFrame끼리의 결합

```cmd
SID-0,D-0,2
SID-0,D-3,1
SID-1,D-10,4
SID-2,D-5,1
SID-2,D-8,1
SID-2,D-20,1
[hadoop@master ~]$ vi dessert-order.csv
[hadoop@master ~]$ hadoop fs -put /home/hadoop/dessert-order.csv /data/DataFrame

```

```scala
case class DessertOrder(sId: String, menuId: String, num: Int)
val dessertOrderRDD=sc.textFile("/data/DataFrame/dessert-order.csv")
val dessertOrderDF=dessertOrderRDD.map{record=>
val splitRecord=record.split(",")
val sId=splitRecord(0)
val menuId=splitRecord(1)
val num=splitRecord(2).toInt
    DessertOrder(sId,menuId,num)
}.toDF

val amntPerMenuPerSlipDF=
dessertDF.join(
dessertOrderDF,
dessertDF("menuId")=== dessertOrderDF("menuId"),"inner").select($"sId",$"name",($"num"*$"price") as "amount_per_menu_per_slip")

amntPerMenuPerSlipDF.show

val anmtPerSlipDF=amntPerMenuSlipDF.groupBy($"sId").agg(sum($"amount_per_menu_per_slip") as "amount_per_slip").select($"sId",$"amount_per_slip")
anmtPerSlipDF.show

//스파크의 SQL의 UDF이용하기
val strlen=sqlContext.udf.register("strlen",(str:String)=> str.length)
sqlContext.sql("SELECT strlen('Hello 스파크 SQL') AS result_of_strlen").show
```



## Spark SQL

spark -shell 실행

```cmd
[hadoop@master hadoop-2.7.7]$ spark-shell --master local
```

```scala
//########DataFrameWriter와 DataFrameReader로 구조화 데이터 read/write 실습################
case class Dessert(menuId: String, name: String, price: Int, kcal: Int)
val dessertRDD = sc.textFile("/data/dessert-menu.csv") // 이 파일 루트는 csv가 있는 하둡위치로 바꾸자.
val dessertDF = dessertRDD.map{ record => 
    val splitRecord = record.split(",")
    val menuId = splitRecord(0)
    val name = splitRecord(1)
    val price = splitRecord(2).toInt
    val kcal = splitRecord(3).toInt
    Dessert(menuId, name, price, kcal)
    }.toDF
//앞의 내용이 선행되어야 한다.
val dfWriter=dessertDF.write
dfWriter.format("parquet").save("/data/dessert/dessert_parquet")
//이리되면 dessert_parquet디렉토리과 생김과 동시에 Parquet포맷 파일 생성된다.

val dfReader=spark.sqlContext.read
val dessertDF2=dfReader.format("parquet").load("/data/dessert/dessert_parquet")
dessertDF2.orderBy($"name").show(3) //이름순서이기 때문에 ㄱ ㄴㄷ ...순서로 간다.
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|  D-11|고구마 파르페| 6500| 650|
|  D-12|    녹차 빙수| 3800| 320|
|   D-7|  녹차 파르페| 4500| 380|
+------+-------------+-----+----+
only showing top 3 rows


```



#### 테이블 형식의 구조화된 데이터셋 다루기(테이블 형식으로 저장, 조회)

```scala
dessertDF.write.format("parquet").saveAsTable("dessert_tbl_parquet")
//format("parquet")를 테이블 이름 dessert_tbl_parquet으로 저장 후 조회
spark.read.format("parquet").table("dessert_tbl_parquet").show(3)
spark.sql("select*from dessert_tbl_parquet limit 3").show
```

#### 세이프 모드

- 출력 장소에 지정한 데이터셋이 이미 존재할 경우 어떻게 처리할지 결정

| 세이프 모드          | 효과                                                         |
| -------------------- | ------------------------------------------------------------ |
| SaveMode.ErrorExists | 예외를 발생시킨다(디폴트)                                    |
| SaveMode.Append      | 기존 데이터셋에 덧붙인다                                     |
| SaveMode.Overwrite   | 기존 데이터셋을 덮어쓴다.                                    |
| SaveMode.Ignore      | 기존 데이터셋을 변경하지 않는다.(=create tabe if not exists) |

- 세이프 모드 하려면 **org.apache.spark.sql.SaveMode**임포트 해야한다.

```cmd
hadoop fs -mkdir /output/dessert_json
```

```scala
dessertDF.write.save("/output/dessert_json") //예외 AnalysisException 발생 (이미 존재함)

```

```scala
import org.apache.spark.sql.SaveMode
dessertDF.write.format("json").mode(SaveMode.Overwrite).save("output/dessert_json") //예외발생 안한다!

val dessertDF2=dfReader.format("json").load("/output/dessert_json")
dessertDF2.orderBy($"kcal").show(4)
+------+---------------+-----+----+
|menuId|           name|price|kcal|
+------+---------------+-----+----+
|  D-15|  커스터드 푸딩| 2000| 120|
|   D-8|  바닐라 젤라또| 3600| 131|
|  D-17|    치즈 수플레| 2200| 160|
|  D-14|바나나 크레이프| 3300| 220|
+------+---------------+-----+----+
only showing top 4 rows

```



#### 명시적으로 스키마 정보 부여하기

- 데이터에 대한 스키마 정보를 나타내는 API, StuctType은 데이터프레임의 레코드에 대한 구조 정보를 나타내며, 내부에 여러 개의 StructField를 갖는 형태로 정의

```scala
import java.math.BigDecimal
case class DecimalTypeContainer(data: BigDecimal)
val bdContainerDF=sc.parallelize(
List(new BigDecimal("123456.6789999999999"))).map(data=>DecimalTypeContainer(data)).toDF
bdContainerDF.printSchema

bdContainerDF.show(false)//data문자열의 길이가 20을 넘을 경우에도 생략하지 않고 표시하려는 것
+--------------------+
|data                |
+--------------------+
|123456.6789999999999|
+--------------------+

bdContainerDF.write.format("orc").save("/output/bdContainerORC")
val bdContainerORCDF=spark.sqlContext.read.format("orc").load("/output/bdContainerORC")
bdContainerORCDF.printSchema
bdContainerORCDF.show(false)
+-------------------------+
|data                     |
+-------------------------+
|123456.678999999999900000|
+-------------------------+

bdContainerDF.write.format("json").save("/output/bdContainerJSON")
val bdContainerJSONDF= spark.sqlContext.read.format("json").load("/output/bdContainerJSON")
bdContainerJSONDF.printSchema

bdContainerJSONDF.show(false)
+----------+
|data      |
+----------+
|123456.679|
+----------+

import org.apache.spark.sql.types.DataTypes._
val schema=createStructType(Array(createStructField("data",createDecimalType(38,18),true)))

val bdContainerJSONDF=spark.sqlContext.read.schema(schema).format("json").load("/output/bdContainerJSON")
bdContainerJSONDF.printSchema

bdContainerJSONDF.show(false)
+-------------------------+
|data                     |
+-------------------------+
|123456.678999999999900000|
+-------------------------+
//이처럼 스키마를 명시적으로 설정함으로 형변환에 따른 오차 없이 파일 내용을 정확히 DataFrame으로 변환 가능
```

#### 파티셔닝

- where절이나 where 메서드의 필터링 조건식에 파티션을 지정하여 해당 파티션 이외의 데이터를 읽지 않게 함

```scala
import org.apache.spark.sql.types.DataTypes._
val priceRangeDessertDF=dessertDF.select(((($"price"/1000)cast IntegerType)*1000)as "price_range",dessertDF("*"))



//priceRangeDessertDF를 파티셔닝하지 않고 출력
priceRangeDessertDF.write.format("parquet").save("/output/price_range_dessert_parquet_non_partitioned")

val nonPartitionedDessertDF=spark.sqlContext.read.format("parquet").load("/output/price_range_dessert_parquet_non_partitioned")
nonPartitionedDessertDF.where($"price_range">=5000).explain
== Physical Plan ==
*(1) Project [price_range#243, menuId#244, name#245, price#246, kcal#247]
+- *(1) Filter (isnotnull(price_range#243) && (price_range#243 >= 5000))
   +- *(1) FileScan parquet [price_range#243,menuId#244,name#245,price#246,kcal#247] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/output/price_range_dessert_parquet_non_partitioned], PartitionFilters: [], PushedFilters: [IsNotNull(price_range), GreaterThanOrEqual(price_range,5000)], ReadSchema: struct<price_range:int,menuId:string,name:string,price:int,kcal:int>

```

```scala
//priceRangeDessertDF를 파티셔닝하고 출력
priceRangeDessertDF.write.format("parquet").partitionBy("price_range").save("/outputprice_range_dessert_parquet_partitioned")
val partitionedDessertDF=spark.sqlContext.read.format("parquet").load("/outputprice_range_dessert_parquet_partitioned")
partitionedDessertDF.where($"price_range">=5000).explain
                                                                                                     == Physical Plan ==
*(1) FileScan parquet [menuId#259,name#260,price#261,kcal#262,price_range#263] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/outputprice_range_dessert_parquet_partitioned], PartitionCount: 2, PartitionFilters: [isnotnull(price_range#263), (price_range#263 >= 5000)], PushedFilters: [], ReadSchema: struct<menuId:string,name:string,price:int,kcal:int>

```

#### 구조화된 데이터셋을 테이블로 다루기

`create table 테이블명 using <해당 포맷의 프로바이더명>options(옵션,옵션....)

- using절에는 다루고자 하는 포맷에 대응하는 프로바이더의 이름 지정 . 아래를 참고

| 파일 포맷 | 대응 프로바이더명             |
| --------- | ----------------------------- |
| Parquet   | org.apache.spark.sql.parquet  |
| ORC       | org.apache.spark.sql.hive.orc |
| JSON      | org.apache.spark.sql.json     |

```cmd
[hadoop@master ~]$ spark-sql
#하면 들어가진다 spark-sql
```

```scala
create table dessert_tbl_json USING org.apache.spark.sql.json OPTIONS(path 'output/dessert_json');

SELECT name, price FROM dessert_tbl_json LIMIT 3;
초콜릿 파르페   4900
푸딩 파르페     5300
딸기 파르페     5200

```

#### 테이블 캐시

- DataFrame이나 테이블을 이그제큐터에 캐시 가능. 

```scala
df.cache()// DataFrame을 캐시에 저장, df는 DataFrame을 나타낸다.
spark.sqlContext.cacheTable("tbl")//테이블을 캐시에 저장, sqlContext는 SQLContext의 인스턴스를 tbl은 테이블명을 나타낸다.

df.unpersist// DataFrame 캐시에서 삭제
spark.sqlContext.uncacheTable("tbl")//테이블을 캐시에서 삭제

```

## 스파크 스트리밍

#### 스트림처리

- 7장에서는 스트림처리를 구현하기 위해 스트리밍 라이브러리가 있다.

- 짧은 시간 발생한 데이터를 반복적으로 처리하는 것 

- 예를 들어 혈액냉장고의 온도 센서 확인 등

- 갑작스러운 변화의 경향 파악이 쉽다.

  

#### 스트림 데이터 란?

- 스토리지에 저장된 하나의 큰 데이터가 아닌 '반영구적으로 계속 생성되는 데이터'
- 짧은 시간 간격으로 생성되는 것이 특정

### 스파크 스트리밍

- RDD변환처리로 데이터처리 구현
- 조각을 나누어 입력 데이터를 구성, 반복 처리한다는 것이 특징

#### DStrema 변환

1. map
   - 원래의 DStream을 변환하여 새로운 DStream 생성하는 API다.
2. flatMap
   - DStream의 각 요소에 대해서 함수를 적용하고, 다차원 컬렉션을 한 차원 풀어 내린 요소들로 이루어진 DStream생성. 인수로는 변환처리가 정의된 함수를 건네준다.
3. filter
   - 원래의 DStream에서 조건을 만족하는 요소만을 남긴 DStream을 생성. 인수로는 조건을 확인하기 위한 함수를 건네준다.

#### 스파크 스트리밍 동작 확인

1. 

#### 넷캣(Netcat)

1. TCP나 UDP프로토콜을 사용하는 네트워크 연결에서 데이터를 읽고 쓰는 간단한 유틸리티 프로그램

   nc는 network connection에 읽거나 쓴다.

   Network connection에서 raw-data read,write를 할 수 있는 유틸리티프로그램으로 원하는 포트로 원하는 데이터를 주고받을 수 있는 특징으로  해킹에도 사용되며 컴퓨터 포렌식에 있어서 라이브시스템의 데이터를 손쉽게 가져오기 위해 사용

```cmd
#마스터에서
[hadoop@master ~]$ nc -l 9999
#슬레이브에서
[hadoop@slave ~]$ nc 마스터ipaddress(000.000.000.000) 9999
#하면 둘이 연결이 된다! 아무거나 작성해 보자

#마스터에서 (파일은 만들어 놓아야 한다.)
nc -l 9999 > ./listen.txt
#슬레이브에서
nc 192.168.255.130 9999 < ./input.txt


```

- 마스터에서 ` nc -lk 9999' 상태에서(slave연결 끊고)
- /usr/local/hadoop2.7.7/ 에서 spark-shell (--master local[*])실행 후

```scala
import org.apache.spark.{SparkContext,SparkConf}
import org.apache.spark.streaming.{Seconds,StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level,Logger}

Logger.getRootLogger.setLevel(Level.WARN)

val ssc=new StreamingContext(sc,Seconds(10))
val lines=ssc.socketTextStream("localhost",9999,StorageLevel.MEMORY_AND_DISK_SER)
val words=lines.flatMap(_.split(" "))
val wordCounts=words.map((_, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()


```

2. 클러스터 환경에 애플리케이션 배포

```scala
//hadoop fs -mkdir /data/sample_dir
import org.apache.spark.{SparkContext,SparkConf}
import org.apache.spark.streaming.{Seconds,StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level,Logger}

Logger.getRootLogger.setLevel(Level.WARN)

val ssc=new StreamingContext(sc,Seconds(10))
val lines=ssc.textFileStream("/data/sample_dir/")
val words=lines.flatMap(_.split(" "))
val wordCounts=words.map((_, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()

//hadoop fs -put /usr/local/spark/README.md /data/sample_dir
//대상 파일을 읽어서 wordCount처리 하는 중! 파일이 업로드 되면 바로 읽는다.
```

#### 평균데이터 계산

- 스파크 스트리밍을 이용해 샘플 데이터로 
- [데이터](http://archive.ics.uci.edu/ml/index.php) 에서 human activity ~를 data folder로 zip으로 받자

```cmd
[hadoop@master Downloads]$ unzip UCI\ HAR\ Dataset.zip
#파일이 이름에 공백이 있으므로 unzip으로 풀어주자.
[hadoop@master Downloads]$ mv UCI\ HAR\ Dataset UCI_HAR_Dataset
#공백이 있어 불편함으로 이름을 바꿔주자 UCI_HAR_Dataset으로 이름이 바뀐것을 확인한다.



```



## 머신러닝:MLlib

- MLlib은 통계 처리나 머신러닝을 구현하기 위한 라이브러리로, 내부적으로 스파크 코어의 기본 API를 이용하므로 분산처리 기능 자연스럽게 사용 가능.
- 스파크 코어에는 스파크 SQL, MLlib, 스파크 스트리밍, 그래프 X가 있다.

#### 통계 처리, 머신러닝

- 특정 데이터로부터 수학적 기법을 이용하여 그 성질을 끄집어내는 처리
- 과거의 데이터를 이용하여 미래 데이터에 대한 예측을 하는 처리
- 대부분 행렬계산 라이브러리
- 예로는 
  1. 텍스트 정보로부터 긍정적/부정적 표현 여부 판정
  2. 인터넷 쇼핑몰 등에서 사용자의 일정한 패턴을 부석하여 상품 추천
  3. 등등
- [스파크공식사이트](https://spark.apache.org/)에서 최신 정보가 추가 되고 있다.

#### 기본 요소

1. 벡터(Vector)- 1차원 데이터 다루는 데이터 타입

   - LocalVector

     > 기본적인 벡터 타입으로 밀집 벡터와 희소 벡터 클래스를 이용

   - LabeledPoint

     > LocalVector+레이블 
     >
     > 알고리즘이용시 독립변수와 종속 변수 함께 보존시 사용

2. 행렬(Matrix)-1개 이상의 벡터로 구성되는 행렬 형식 데이터 타입

   - LocalMatrix

     > 한 대의 호스트로 행렬 형식 데이터 다루기 위한 데이터 타입

   - DistributedMatrix

     > 대규모 데이터셋을 분산처리하기 위한 행렬 형식 데이터 타입.

#### 분류와 회귀

- 미리 학습해둔 학습 데이터를 통해 분류모델 작성, 따로 처리된 데이터에 모델 을 적용할 수 있다. 

#### 협업 필터링

- 고객/ 상품에 관한 구매 이력 등의 데이터를 입력데이터로 작성하고 이를 이용하여 관련 정보 추천을 한다.

#### 클러스터링

- 데이터가 주어졌을 때 여러 개의 집합으로 나누는 것. 예를 들어 정보성 메일을 받을 때 고객의 특성을 바탕으로 분류하고 이에 적합한 메일을 보내는 것

#### 차원 축소

- 차원 축소 방법으로 SVD,PCA를 이용 할 수 있다. 이를 이용하여 어떤 정보든 형태적으로 유지하면서 데이터 차원을 축소 가능
- 이해하기 쉬운 저차원으로 변환하기 위해, 차원이 너무 높이 알고리즘 분석이 어려운 경우 사용

#### 특징 추출/변환

- 문서 특징 추출하는 방법중 하나로 벡터화 방법으로 TF-IDF, Word2Vec를 사용 가능.
- 수치 데이터 뿐만이 아닌 텏트 데이터도 입력으로 사용가능

#### 빈발 패턴 마이닝

- FP-growth, 연관성 규칙 마이닝, 순차 패턴 마이닝 등을 사용 가능. 이를 이용하여 '빵을 사는 사람은 우유를 사는 경우가 많다' 와 같은 패턴 발견 가능. 

#### PMML 익스포트 기능

#### PMML익스포트 기능

- 스파크 이외의 툴에서 이 모델을 사용할 수 있게 하는 것.

#### K-means 개요

- K-means의 샘플용 데이터는 스파크 본체에 포함

```cmd
cd usr/local/spark
cd data/mllib
cat kmeans_data.txt
```

- 로 확인 가능

```scala
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
val sparkHome=sys.env("SPARK_HOME")
val data=sc.textFile("file://"+sparkHome+"/data/mllib/kmeans_data.txt")
val parsedData=data.map{s=>
Vectors.dense(s.split(' ').map(_.toDouble))}.cache()


val numClusters=2
val numIterations=20
val clusters=KMeans.train(parsedData,numClusters,numIterations)

clusters.k

clusters.clusterCenters

val vec1=Vectors.dense(0.3,0.3,0.3)
clusters.predict(vec1)

val vec2=Vectors.dense(8.0,8.0,8.0)
clusters.predict(vec2)

parsedData.foreach(vec=>
                  println(vec+"=>"+clusters.predict(vec)))

val predictedLabels=parsedData.map(vector=> clusters.predict(vector))
predictedLabels.saveAsTextFile("output/kmeans")
//경로 지정을 안했으므로 하둡에 /user/hadoop/output/kmenas에 있다.
clusters.save(sc, "kmeans_model")


```

```cmd
[hadoop@master ~]$ hadoop fs -cat /user/hadoop/kmeans_model/metadata/part-00000 
{"class":"org.apache.spark.mllib.clustering.KMeansModel","version":"2.0","k":2,"distanceMeasure":"euclidean","trainingCost":0.11999999999994547}
#현재 사용하는 모델의 조건? 등을 알 수 있다.
```



##### 만약 클러스터의 갯수를 모른다면

```scala
val WSSSE=clusters.computeCost(parsedData)
println("Within Set Sum of Squared Errors="+WSSSE) Within Set Sum of Squared Errors=0.11999999999994547
```

#### 단어의 벡터화(한국어)

##### 문서에서 단어 추출(형태소 분석)

- 한국어는 단순한 공백으로 분할로는 충분하지 않기에 의미를 갖는 최소한의 단위인 형태소로 분할해야 한다.
- 형태소 분석은 MLlib에 포함되어있지는 않다.
- twitter-korean-text를 이용한다.

[트위터메이븐 한글 자르파일](https://mvnrepository.com/artifact/com.twitter.penguin/korean-text/4.4.4) 다운!

```cmd
$spark_HOME/jars폴더에 mv
```



```scala
libraryDependencies += "com.twitter.penguin" %% "korean_text" % "4.0"
```

```scala
spark-shell --master yarn \
--packages com.twitter.penguin:korean-text:4.4.4

spark-shell \
--master yarn \
--packages com.twitter.penguin:korean-text:4.4.4 \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer
// 위에 \표시는 엔터 표시임으로 \ 지우고 한줄로 작성해도 된다spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.0 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
//안되면 yarn말고 local로 하자
import java.io.StringReader
import org.apache.spark.mllib.feature.{Word2Vec,Word2VecModel}
import com.twitter.penguin.korean.TwitterKoreanProcessor
import com.twitter.penguin.korean.tokenizer.KoreanTokenizer.KoreanToken
import org.apache.spark.mllib.linalg.Vectors

val sentence="이 책은 무슨 책 입니까"
val normalized: CharSequence=TwitterKoreanProcessor.normalize(sentence)

//토크나이즈
val tokens: Seq[KoreanToken]=TwitterKoreanProcessor.tokenize(normalized)

//어근 추출
val stemmed: Seq[KoreanToken]=TwitterKoreanProcessor.stem(tokens)
```





##### 실제 파일로 찾아보자

```scala
val input=sc.textFile("")
```



spark-shell --master local --packages com.twitter.penguin:korean-text:4.4.4 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer

spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.4.4

#  참고로 알자

#### 파일 찾기

```cmd
[hadoop@master ~]$ hadoop fs -find / -name kmeans -print
```







