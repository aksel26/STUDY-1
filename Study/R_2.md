### 		6.  분류분석

1. 다수의 변수를 갖는데이터 셋을 대상으로 특정 변수의 갑승ㄹ 조건으로 지정하여 데이터를 분류
2. 종류
   - 의사결정트리(Decision Tree)
   - 랜덤 포레스트(Random Forest)
   - 인공신경망(Artificial Neural Network)
3. 기존 고객들의 데이터를 활용, 분류모델을 생성한 후 새로운 고개에 대해 예측

##### 6.1 특성

1. Y 변수 존재 : 설명변수(x 변수)와 반응변수(y 변수)가 존재한다.
2. 의사결정트리 : 분류 예측모델에 의해서 의사결정트리 형태로 데이터가 분류된다.
3. 비모수 검정 : 선형성, 정규성, 등분산성 가정이 필요 없다
4. 추론 기능 없다. : 유의수준 판단 기준이 없다 (추론 기능 없음)
5. 활용분야 : 이탈고객과 지속고객 분류, 신용상태의 좋고, 나쁨, 번호이동고객과 지속 고객 분류 등

##### 6.2 절차

1. 학습 데이터 생성
2. 분류 알고리즘을 통해 예측 모델 생성
3. 검정 데이터를 통해 분류규칙의 모델 평가(모형 평가)
4. 새로운 데이터에 적용하여 결과 예측

> **모형평가 ?**
>
> 어떤 모형이 random하게 예측하는 모형보다 예측력이 우수한지, 고려된 여러 모형 중 어느 모형이 가장 좋은 예측력을 보유하고 있는지를 비교/분석하는 과정.
>
> =어떤 모델이 좋은지 평가

##### 6.3 의사결정 트리

1. 나무(Tree) 구조 형태로 분류결과를 도출

2. 입력변수 중 가장 영향력 있는 변수를 기준으로 이진분류하여 분류 결과를 나무 구조 형태로 시각화

3. 비교적 모델 생성이 쉽고, 단순, 명료하여 현업에서 많이 사용되는 지도학습 모델

4. 의사결정규칙을 도표화 하여 분류와 예측을 수행하는 분석방법

   - party 패키지 ctree()

     >` 1)  Ozone <= 37; criterion= 1,
     >statistic=56.086`
     >
     >- 첫번째 번호는 반응변수(종속변수)에 대해서 설명변수(독립변수)가 영향을 미치는 중요 변수의 척도를 나타내는 수치로서 수치가 작을 수록 영향을 미치는 정도가 높고, 순서는 분기되는 순서를 의미한다.
     >- •두번째는 의사결정 트리의 노드명 (노드 번호 뒤에 * 기호가 오면 해당 노드가 마지막 노드를 의미)
     >   노드명 뒤에 해당 변수의 임계값이 조건식으로 온다. 마지막노드이면 조건식이 없다.
     >- 세번째는 노드의 분기 기준(criterion)이 되는 수치
     >- 네번째는 반응변수(종속변수)의 통계량(statistic)이 표시된다. 

     ######   의사결정트리(ctree()) 예시

   ```r
   #의사결정트리
   install.packages("party")
   library(party)
   library(datasets)
   #뉴욕 대기질 측정한 데이터셋
   > str(airquality) #관측치 153 변수 6개
   'data.frame':	153 obs. of  6 variables:
    $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
    $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
    $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
    $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
    $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
    $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...
   
   
   ##온도에 영향을 미치는 변수를 알아보자.###########################
   > formula<-Temp ~ Solar.R+Wind+Ozone
   > #분류모델 생성
   > air_ctree<-ctree(formula, data=airquality)
   > air_ctree
   
   	 Conditional inference tree with 5 terminal nodes
   
   Response:  Temp 
   Inputs:  Solar.R, Wind, Ozone 
   Number of observations:  153 
   
   1) Ozone <= 37; criterion = 1, statistic = 56.086
     2) Wind <= 15.5; criterion = 0.993, statistic = 9.387
       3) Ozone <= 19; criterion = 0.964, statistic = 6.299
         4)*  weights = 29 
       3) Ozone > 19
         5)*  weights = 69 
     2) Wind > 15.5
       6)*  weights = 7 
   1) Ozone > 37
     7) Ozone <= 65; criterion = 0.971, statistic = 6.691
       8)*  weights = 22 
     7) Ozone > 65
       9)*  weights = 26 
   
   #시각화해보자
   plot(air_ctree)
   ```

   ![1569199373622](R_2.assets/1569199373622.png)

    ```r
   #가장 큰 영향 변수는 Ozone
   #두번쨰는 Wind
   #오존량 37이하이면서 바람의 양이 15.5 이상이면 평균온도 63정도
   #바람의 양이  15.5이하인 경우 평균 온도 70 이상으로 나타난다.
   #태양광은 온도에 영향을 미치지 않는다.
    ```

   ###### ctree() 2번째 

   ```r
   #iris데이터 셋
   #학습데이터와 검정데이터로 나눌것이다.
   #Sample()
   set.seed(1234)#시드값적용 랜덤값이 동일하게 생성
   idx<-sample(1:nrow(iris),nrow(iris)*0.7)
   train<-iris[idx,]#학습데이터
   test<-iris[-idx,]#검정데이터
   #종속변수 (꽃종류임으로 Species, 나머지 4개를 독립변수로)
   str(iris)
   formula<-Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width
   #분류모델 생성성
   iris_ctree<-ctree(formula,data=train)
   iris_ctree
   plot(iris_ctree,type="simple")
   plot(iris_ctree)
   
   ```

   ![1569199891122](R_2.assets/1569199891122.png)

   ```r
   #첫번째 요인 Petal.Length
   #두번째 요인 Petal.Width
   #꽃종 분류에 가장 중요한 독립 변수는 Petal.Length와 Petal.Width다.
   #이 결과값은 Seed값이 다르면 조금씩 달라진다.
   ```

   ###### 의사결정트리 분류모델 평가

   ```r
   > pred<-predict(iris_ctree,test)
   > table(pred,test$Species)
               
   pred         setosa versicolor virginica
     setosa         16          0         0
     versicolor      0         15         1
     virginica       0          1        12
   ```

   ###### 분류정확도 계산

   ```r
   #분류정확도 계산
   # 세토사는 세토사로 버지니카는 버지니카로 분류됐는지 확인
   > (16+15+12)/nrow(test)
   [1] 0.9555556 #정확도는 96%정도 
   ```

   

   - rpart 패키지 rpart()

     > 1. 재귀분할(recursive partitioning)
     > 2. 2수준 요인으로 분산분석을 실행한 결과를 트리 형태로 제공하여 모형을 단순화
     > 3. 전체적인 분류기준을 쉽게 분석할 수 있는 장점이 있다

   ###### rpart 패키지 rpart() 이용한 분류

   ```r
   install.packages("rpart")
   library(rpart)
   data(iris)
   iris.df<-rpart(Species~.,data=iris)
   iris.df
   plot(iris.df)
   text(iris.df,use.n=T,cex=0.6)
   post(iris.df,file="")
   #줄기에 분기 조건
   #끝 노드에는 반으엽ㄴ수의 결과값이 나타남
   #꽃 종류 변수를 분류하는 가장 중요한 변수는 Petal.Length와 Petal.Width다.
   ```

   ![1569206923307](R_2.assets/1569206923307.png)

##### 6.4 k겹 교차 검증

- 테스트를 더 정확하게 설정할수록 세상으로 나왔을 때 더 잘 작동한다고 할 수 있습니다. 
- 딥러닝 혹은 머신러닝 작업을 할 때 늘 어려운 문제 중 하나는 알고리즘을 충분히 테스트하였어도 데이터가 충분치 않으면 좋은 결과를 내기가 어렵습니다
- 데이터의 약 70%를 학습셋으로 써야 했으므로 테스트셋은 겨우 전체 데이터의 30%에 그쳤습니다. 이 정도 테스트만으로는 실제로 얼마나 잘 작동하는지 확신하기는 쉽지 않습니다.
- k겹 교차 검증(k-fold cross validation)  - 테스트 데이터 충분하지 않을 경우  단점을 보완하고자 만든 방법이 바로 입니다. 
- k겹 교차 검증이란 데이터셋을 여러 개로 나누어 하나씩 테스트셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법입니다. 이렇게 하면 가지고 있는 데이터의 100%를 테스트셋으로 사용할 수 있습니다. 

```r
##k겹 교차 검증
# 3겹, 2회 반복을 위한 샘플링
install.packages("cvTools")
library(cvTools)
cross<-cvFolds(nrow(iris),K=3,R=2)
> str(cross)
List of 5
 $ n      : num 150
 $ K      : num 3
 $ R      : num 2
 $ subsets: int [1:150, 1:2] 21 102 134 9 19 22 40 29 109 38 ...
 $ which  : int [1:150] 1 2 3 1 2 3 1 2 3 1 ...
 - attr(*, "class")= chr "cvFolds"
cross # 교차검정 데이터 확인 
Repeated 3-fold CV with 2 replications:    
Fold      1   2
   1     21 133
   2    102 126
   3    134  87
   1      9 109
   2     19 122
   3     22  55
   1     40 105
   2     29  75
   3    109  24
   1     38 150
   2     88  52
   3    107  38
   1     70 101
   2    119 100
   3     79   2
   1     86 123
   2     71 135
   3    123  36
   1    133 114
   2     99  20
   3     26 106
   1    145  17
   2     87  68
   3     28  86
   1     98  10
   2    101  88
   3     48  83
   1    150  77
   2     41  46
   3     35  41
   1    144 134
   2     67  27
   3     72 148
   1    146  79
   2     61  30
   3     68  59
   1     84 124
   2     83  74
   3    148 120
   1     96  33
   2     17  70
   3     25  60
   1    121 127
   2      2  99
   3    125  97
   1     47  49
   2    105  47
   3     30 104
   1     10  85
   2    129   4
   3     57 147
   1     49  14
   2     89  91
   3     80 146
   1     73 129
   2     20  42
   3     12  48
   1     37 145
   2     81  98
   3    137 113
   1    104  96
   2    135 107
   3    141 108
   1     82  15
   2    117  39
   3     18  53
   1     23 119
   2     31 102
   3      6 139
   1      7  13
   2    118 143
   3      4  61
   1     43  54
   2    116 140
   3     15  72
   1    110  51
   2     78  16
   3     39 141
   1     42  81
   2    108  89
   3     58 121
   1     85  19
   2     11  64
   3     56 111
   1      8 138
   2    106 142
   3     97  71
   1     93  31
   2     52 149
   3     94  56
   1     64  76
   2     95 137
   3     33  84
   1     54  95
   2    128  80
   3      3  43
   1     36 118
   2    100 132
   3    115 103
   1    112  93
   2    138  73
   3    143  58
   1      1 128
   2    103  94
   3    127  18
   1     44  22
   2    126  40
   3    149 115
   1    132 131
   2     13  63
   3     77  11
   1    131   3
   2    113 110
   3     50  62
   1     74 130
   2     65  26
   3    122 125
   1     27  66
   2     66  82
   3     69  78
   1    114  21
   2    139  45
   3     24 136
   1     91  32
   2    142  65
   3     60  50
   1     14 144
   2     63 116
   3      5   6
   1     62   8
   2     46   9
   3    147  69
   1    124  23
   2    140  12
   3     16  34
   1     76  35
   2     90   1
   3     51  92
   1     53  67
   2    111  44
   3     45  90
   1    130   7
   2    136  37
   3     55   5
   1     92 112
   2     34 117
   3     75  29
   1    120  28
   2     59  25
   3     32  57
> length(cross$which)
[1] 150
> dim(cross$subsets)
[1] 150   2
> table(cross$which)

 1  2  3 
50 50 50 
> R=1:2 #2회 반복
> K=1:3 #3겹
> CNT=0  #카운트 변수
> ACC<-numeric() #정확도 저장
> for(r in R ){
+     cat('\n R=',r,'\n')
+   for(k in K){
+     datas_idx<-cross$subsets[cross$which==k,r]
+     test<-iris[datas_idx, ] #테스트 데이터 생성
+     cat('test:',nrow(test),'\n')
+     
+     formula<-Species ~.
+     train<-iris[-datas_idx, ] #훈련데이텃 생성
+     cat('train:',nrow(train),'\n')
+     
+     model<-ctree(formula,data=train)
+     pred<-predict(model,test)
+     t<-table(pred,test$Species)  #교차테이블 만들기
+     print(t)  # 혼돈 메트릭스 생성성
+     CNT<-CNT+1
+     #정확도
+     ACC[CNT]<- ((t[1,1]+t[2,2]+t[3.3])/sum(t))   }}

 R= 1 
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         18          0         0
  versicolor      0         17         2
  virginica       0          0        13
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         11          0         0
  versicolor      0         16         1
  virginica       0          2        20
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         19          0         0
  versicolor      2         15         2
  virginica       0          0        12

 R= 2 
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         18          0         0
  versicolor      0         12         2
  virginica       0          0        18
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         16          0         0
  versicolor      2         18         2
  virginica       0          0        12
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         14          0         0
  versicolor      0         15         1
  virginica       0          5        15


> CNT #데스트 데이터 3셋 생성 모델, 예측 비교를 2번 반복, 즉 6회 수행
[1] 11
> ACC # 6회 수행 정확도 확인
 [1] 0.04444444 0.04444444         NA         NA         NA 0.70000000 0.54000000
 [8] 0.68000000 0.60000000 0.68000000 0.58000000
> mean(ACC,na.rm=T) # 6회 정확도 평균!
[1] 0.4836111
```



##### 6.5 랜덤포레스

1. 의사결정트리에서 파생된 앙상블 학습기법을 적용한 모델
2. 앙상블 학습 기법 – 새로운 데이터에 대해서 여러 개의 트리(Forest)로 학습을 수행한 후 학습 결과들을 종합해서 예측하는 모델
3. 기존의 의사결정트리 방식에 비해서 많은 데이터를 이용하여 학습을 수행하기 때문에 비교적 예측력이 뛰어나고, 과적합(overfitting)문제를 해결할 수 있다
4. 과적합 문제 – 작은 데이터 셋은 높은 정확도가 나타나지만 큰 데이터셋에서는 정확도가 떨어지는 현상을 의미

`randomForest(formula, data, ntree, mtry, na.action, importance)`

- formula : y~x 형식으로 반응변수와 설명변수 식
- data : 모델 생성에 사용될 데이터 셋
- ntree :  복원추출하여 생성할 트리 수 지정
- mtry : 자식 노드를 분류할 변수 수 지정
- na.action :  결측치를 제거할 함수 지정
- importance : 분류모델 생성과정에서 중요 변수 정보 제공 여부

###### 6.5.1 실습

```r
#랜덤포레스트 분류 분석
install.packages("randomForest")
library(randomForest)

> data(iris)
> model<-randomForest(Species~.,data=iris)
> model

Call:
 randomForest(formula = Species ~ ., data = iris) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08

#Number of trees는 학습데이터(Forest)로 복원 추출 방식으로  500개 생성했다는 의미
#No, of variables tried at each split는 두 개의 변수를 이용하여 트리의 자식노드가 분류되었다는 의미
#ntree:500, ntry:2 기본으로 설정
#error.rate: 모델의 분류정확도 오차 비율을 의미
#Confusion matrix(혼돈매트릭스) 
> (50+47+46)/nrow(iris)
[1] 0.9533333
> (50+47+46)/150
[1] 0.9533333
#ntree :300, mtry=4, 결측치 제거
> model2<-randomForest(Species~.,data=iris,ntree=300,mtry=4,na.action=na.omit)
> model2

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = 300,      mtry = 4, na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 300
No. of variables tried at each split: 4

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06
> (50+47+47)/nrow(iris)
[1] 0.96



```

###### 6.5. 2 중요변수 생성으로 랜덤포레스트 모델 생성

```r
##### 중요변수 생성으로 랜덤포레스트 모델 생성
> model3<-randomForest(Species~.,data=iris,importance=T,na.action = na.omit)
> model3

Call:
 randomForest(formula = Species ~ ., data = iris, importance = T,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
> (50+47+46)/nrow(iris)
[1] 0.9533333

```

###### 6.5.3.중요 변수 찾기(importance)

```r
#중요변수 보기 importance속성은 분류모델 생성하는 과정에서 입력변수 중 가장 중요한 변수가 어떤 변수인가 알려준다.
> importance(model3)
                setosa versicolor virginica MeanDecreaseAccuracy
Sepal.Length  6.442767   7.426228  9.324005            11.306699
Sepal.Width   4.743591   1.097165  5.131779             5.140809
Petal.Length 19.784523  32.441311 27.979269            32.365475
Petal.Width  24.275826  32.640274 32.723157            35.243491
             MeanDecreaseGini
Sepal.Length        10.130830
Sepal.Width          2.380762
Petal.Length        41.178722
Petal.Width         45.596859
#MeanDecreaseAccuracy 분류정확도를 개선하는데 기여한 변수를 수치로 제공
# 가장 크게 기여한 것은 Petal.Width 
```



###### 6.5.4 최적의 ntree, mtry찾기

```r

#최적의 ntree, mtry수치값 찾기
> for(i in param$n){
+   cat('ntree=',i,'\n')
+   for(j in param$m){
+     cat('mtry=',j,'\n')
+     model_iris<-randomForest(Species~.,data=iris,ntree=i,mtry=j,na.action=na.omit)
+     print(model_iris)
+     
+   }
+ }
ntree= 400 
mtry= 2 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 400
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
mtry= 3 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 400
No. of variables tried at each split: 3

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
mtry= 4 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 400
No. of variables tried at each split: 4

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
ntree= 500 
mtry= 2 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
mtry= 3 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 3

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06
mtry= 4 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 4

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
ntree= 600 
mtry= 2 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 600
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06
mtry= 3 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 600
No. of variables tried at each split: 3

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06
mtry= 4 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 600
No. of variables tried at each split: 4

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
```

### 7. 인공신경망

- 인간의 두뇌 신경(뉴런)들이 상호작용하여 경험과 학습을 통해서 패턴을 발견하고,  발견된 패턴을 통해서 특정 사건을 일반화하거나 데이터를 분류하는데 이용되는 기계학습 방법
- 인간의 개입 없이 컴퓨터가 스스로 인지하고 추론, 판단하여 사물을 구분하거나 특정 상황의 미래를 예측하는데 이용될 수 있는 기계학습 방법
- 문자, 음성, 이미지 인식, 증권시장 예측, 날씨 예보 등 다양한 분야에서 활용
  예) 구글의 알파고(딥러닝)

##### 7.1 생물학적 신경망과 인공신경망 비교

| 생물학적 신경망 | 인공신경망 |
| --------------- | ---------- |
| 세포체          | 노드       |
| 수상돌기        | 입력       |
| 축삭            | 출력       |
| 시냅스          | 가중치     |

- 인간의 생물학적 신경세포가 하나가 아닌 다수가 연결되어 의미 있는 작업을 하듯, 인공신경망의 경우도 개별 뉴런들을 서로 시냅스를 통해 서로 연결시켜서 복수개의 계층(layer)이 서로 연결되어 각 층간의 연결 강도는 가중치로 수정(update) 가능. 이와 같이 다층 구조와 연결강도로 학습과 인지를 위한 분야에 활용된다.

##### 7.2 가중치 적용

- 시냅스에서는 외부 신호 입력에 따라서 세기를 적용한다.
- 입력값 (x1, x2, x3,…투)은 수상돌기에 해당하는 외부 신경 자극에 해당하고, 가중치 (w1, w2, w3, ….주) 는 시냅스에 의해서 신호의 세기가 결정되는 부분에 해당한다.
  입력 신호(X)와 일대일로 가중치(W)가 적용된다.
- 경계값(b:bias)은 활성 함수에 의해서 망의 총합을 다음 계층으로 넘길 때 영향을 주는 값이다.
- 입력 신호의 가중치는 중요 변수에 따라서 가중치가 달라지는데, 초기 가중치는 무작위(Random)로 생성되지만, 출력 값의 예측결과에 따라서 가중치는 수정(중요 변수의 가중치는 높게 설정)된다. 

##### 7.3 활성 함수

- 망의 총합과 경계값(bias)을 계산하여 출력 신호(y)를 결정한다.
- 활성 함수는 0과 1사이의 확률분포를 갖는 시그모이드 함수(Sigmoid function)를 이용한다
- 시그모이드 함수는 가중치나 경계값(bias)이 변경된 경우 출력 신호에 변화를 준다.
- 스텝 함수(Step function)는 0 또는 1의 이항값으로 출력 신호(y)가 결정하므로 가중치와 경계값의 변화에 대해서 출력 신호에 변화를 주지 못한다.
- 스텝 함수는 x 변량이 0보다 큰 경우 1, 0보다 적으면 0으로 극단적인 상황만 제공
- 시그모이드 함수는 0과 1 사이의 확률분포를 제공하여 가중치나 바이어스(bias) 변화 시 출력 신호(y)에 변화를 준다

##### 7.4 퍼셉트론(=인공신경망)

- 여러 개의 계층으로 다층화하여 만들어진 인공신경망
- 퍼셉트론 모형에서 입력변수와 출력변수는 분석자가 지정해준다.
- 인공신경망은 은닉층에서 연산과정이 공개되지 않기 때문에 **블랙박스 모형**으로 분류되기도 한다.
- 데이터 분류나 예측 결과는 제공하지만 어떠한 원인으로 결과가 도출되었는지에 대한**이유를 설명할 수 없는 모형**
- 여러 변수 간의 관계를 밝히는데 이 모형을 이용할 수 없다.

##### 7.5 구성 요소

- 여러 개의 계층으로 다층화하여 만들어진 인공신경망
- 퍼셉트론 모형에서 입력변수와 출력변수는 분석자가 지정해준다.
- 인공신경망은 은닉층에서 연산과정이 공개되지 않기 때문에 블랙박스 모형으로 분류되기도 한다.
- 데이터 분류나 예측 결과는 제공하지만 어떠한 원인으로 결과가 도출되었는지에 대한 이유를 설명할 수 없는 모형
- 여러 변수 간의 관계를 밝히는데 이 모형을 이용할 수 없다.

##### 7.6 역전파알고리즘

[쉽게 웹으로 읽어보자 책을](http://thebook.io)

- neuralnet 패키지는 역전파(Backpropagation) 알고리즘을 적용할 수 있고, 가중치 망을 시각화하는 기능도 제공
-  출력변수(y)는 ‘yes’, ‘no’ 형태의 문자열이 아닌 1과 0의 수치형 이여야 한다.

`nnet::neuralnet(formula, data, hidden=1,threshold=0.01, stepmax=1e+05, rep=1,
                   startweights = NULL, learnningrate=NULL, algorithm=“rprop+”)`

- formula : y ~ x 형식으로 반응변수(종속변수)와 설명변수(독립변수) 식
- data : 모델 생성에 사용될 데이터 셋
- hidden : 은닉층(hidden layer)의 수 지정
- threshold : 경계값 지정
- stepmax : 인공신경망 학습을 위한 최대 스텝 지정
- rep : 인공신경망의 학습을 위한 반복 수 지정
- startweights : 랜덤으로 초기화된 가중치를 직접 지정
- learningrate : backpropagation 알고리즘에서 사용될 학습비율을 지정
- algorithm : backpropagation과 같은 알고리즘 적용을 위한 속성

###### 7.6.1 실습

```r
install.packages("nnet")
library(nnet)
#x는 입력 변수
#y는 출력 변수
df=data.frame(x2=c(1:6),x1=c(6:1),y=factor(c('no','no','no','yes','yes','yes')))
str(df)
'data.frame':	6 obs. of  3 variables:
 $ x2: int  1 2 3 4 5 6
 $ x1: int  6 5 4 3 2 1
 $ y : Factor w/ 2 levels "no","yes": 1 1 1 2 2 2
> #인공신경망 모델생성
> model_net<-nnet(y~.,df,size=1)
# weights:  5
initial  value 4.746658 
iter  10 value 2.318645
final  value 0.000071 
converged
#5개의 가중치가 생겼다.
#오차율은 점진적으로 줄어들고 있다.
> model_net
a 2-1-1 network with 5 weights
inputs: x2 x1 
output(s): y 
options were - entropy fitting 

#5개의 가중치를 사용해고 입력값은 x2,x1 출력값은 y임을 확인.
#신경망(a 2-1-1)는 (경계값 -입력변수 -은닉층-출력변수) 형태로 다섯개의 가중치를 보여준다. 

#가중치 요약정보 확인
> summary(model_net)
a 2-1-1 network with 5 weights
options were - entropy fitting 
 b->h1 i1->h1 i2->h1 
  0.38  11.00 -10.96 
#입력층의 경계값(b) 1개 와 입력변수(i1,i2)2개가 은닉층(h1)으로 연결되는 가중치
  b->o  h1->o 
-11.43  22.69 
#은닉층의 경계값(b) 1개와 은닉층의 결과값이 출력층(o)으로 연결되는 가중치

> model_net$fitted.values #분류모델의 적합값 확인
          [,1]
1 1.088253e-05
2 1.088253e-05
3 1.088955e-05
4 9.999871e-01
5 9.999871e-01
6 9.999871e-01
> #분류모델의 예측치 생성, 정확도 확인
#type="class"는 예측 결과를 출력변수 y의 범주('no','yes')로 분류
> p<-predict(model_net,df,type="class")
> table(p,df$y)
     
p     no yes
  no   3   0
  yes  0   3



##########################################iris데이터로 인경신경망
> ###iris데이터 셋 인공신경망 모델 생성
> idx<-sample(1:nrow(iris),0.7*nrow(iris))
> training<-iris[idx,]
> testing<-iris[-idx,]
> model_net_iris<-nnet(Species~.,training,size=1)
# weights:  11
initial  value 116.969821 
iter  10 value 47.684448
iter  20 value 47.105289
final  value 47.104651 
converged
> model_net_iris3<-nnet(Species~.,training,size=3)
# weights:  27
initial  value 126.879341 
iter  10 value 42.405769
iter  20 value 2.480654
iter  30 value 0.546225
iter  40 value 0.002310
final  value 0.000067 
converged

> #입력변수의 값들이 일정하지 않으면 과적합(overfitting)을 피하기 위해 정규화 과정 필요
> #가중치 확인
> summary(model_net_iris)
a 4-1-3 network with 11 weights
options were - softmax modelling 
 b->h1 i1->h1 i2->h1 i3->h1 i4->h1 
  9.17   8.44  40.28 -75.94 -34.66 
 b->o1 h1->o1 
 -9.78  19.65 
 b->o2 h1->o2 
  4.79  -9.10 
 b->o3 h1->o3 
  4.85  -9.54 
> summary(model_net_iris3)
a 4-3-3 network with 27 weights
options were - softmax modelling 
  b->h1  i1->h1  i2->h1  i3->h1  i4->h1 
  16.30    9.54   57.99 -128.50  -69.28 
  b->h2  i1->h2  i2->h2  i3->h2  i4->h2 
 -12.74  -60.97  -41.12  -17.55   -2.32 
  b->h3  i1->h3  i2->h3  i3->h3  i4->h3 
-152.30   -6.89    2.30   22.50   45.39 
  b->o1  h1->o1  h2->o1  h3->o1 
 -33.10   61.45   -6.89  -12.05 
  b->o2  h1->o2  h2->o2  h3->o2 
  68.21 -111.28   16.10  -71.40 
  b->o3  h1->o3  h2->o3  h3->o3 
 -34.60   49.42   -9.30   83.08 





```

###### 7.6.2 정확도 평가

```r
> #분류모델의 정확도 평가
> table(predict(model_net_iris,testing,type="class"),testing$Species)
           
            setosa versicolor virginica
  setosa        13          0         0
  virginica      0         17        15
> table(predict(model_net_iris3,testing,type="class"),testing$Species)
            
             setosa versicolor virginica
  setosa         13          0         0
  versicolor      0         16         3
  virginica       0          1        12
```

###### 7.6.3 실습 2

- iris데이터 셋에 인공신경망 모델 생성: neuralnet 패키지

```r
#####iris 데이터 셋에 인공신경망 모델 생성: neuralnet 패키지 ########
install.packages("neuralnet")
library(neuralnet)
idx<-sample(1:nrow(iris), 0.7*nrow(iris))
training <- iris[idx, ]
testing <- iris[-idx, ]

※neuralnet()함수는 종속변수(출력변수y)가 수치형이어야 합니다.

training$Species2[training$Species=='setosa'] <- 1
training$Species2[training$Species=='versicolor'] <- 2
training$Species2[training$Species=='virginica'] <- 3
head(training)
    Sepal.Length Sepal.Width Petal.Length Petal.Width Species2
99           5.1         2.5          3.0         1.1        2
2            4.9         3.0          1.4         0.2        1
11           5.4         3.7          1.5         0.2        1
128          6.1         3.0          4.9         1.8        3
29           5.2         3.4          1.4         0.2        1
140          6.9         3.1          5.4         2.1        3

testing$Species2[testing$Species=='setosa'] <- 1
testing$Species2[testing$Species=='versicolor'] <- 2
testing$Species2[testing$Species=='virginica'] <- 3
head(testing)
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species2
5           5.0         3.6          1.4         0.2        1
6           5.4         3.9          1.7         0.4        1
10          4.9         3.1          1.5         0.1        1
19          5.7         3.8          1.7         0.3        1
21          5.4         3.4          1.7         0.2        1
23          4.6         3.6          1.0         0.2        1

# 정규화 함수를 이용하여 학습데이터와 검정데이터를 정규화
# 0과 1사이의 범위로 컬럼값을 정규화 
normal <- function(x){
     return ((x-min(x))/(max(x)-min(x)))
}

training$Species <- NULL
testing$Species <-NULL

training_nor <- as.data.frame(lapply(training, normal))
>summary(training_nor)

  Sepal.Length     Sepal.Width      Petal.Length      Petal.Width     
 Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000  
 1st Qu.:0.2353   1st Qu.:0.3333   1st Qu.:0.06897   1st Qu.:0.08333  
 Median :0.4118   Median :0.4167   Median :0.51724   Median :0.50000  
 Mean   :0.4261   Mean   :0.4353   Mean   :0.43186   Mean   :0.42857  
 3rd Qu.:0.5882   3rd Qu.:0.5417   3rd Qu.:0.65517   3rd Qu.:0.66667  
 Max.   :1.0000   Max.   :1.0000   Max.   :1.00000   Max.   :1.00000  
    Species2     
 Min.   :0.0000  
 1st Qu.:0.0000  
 Median :0.5000  
 Mean   :0.4619  
 3rd Qu.:1.0000  
 Max.   :1.0000  

testing_nor <- as.data.frame(lapply(testing, normal))
> summary(testing_nor)

  Sepal.Length     Sepal.Width      Petal.Length     Petal.Width    
 Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.:0.3143   1st Qu.:0.3529   1st Qu.:0.1228   1st Qu.:0.1250  
 Median :0.4571   Median :0.5294   Median :0.6842   Median :0.5833  
 Mean   :0.4756   Mean   :0.5216   Mean   :0.5466   Mean   :0.5269  
 3rd Qu.:0.6571   3rd Qu.:0.7059   3rd Qu.:0.7895   3rd Qu.:0.7917  
 Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
    Species2     
 Min.   :0.0000  
 1st Qu.:0.0000  
 Median :0.5000  
 Mean   :0.5889  
 3rd Qu.:1.0000  
 Max.   :1.0000  


#인공신경망 분류 모델 생성
model_net <- neuralnet(Species2 ~ Sepal.Length+ Sepal.Width+Petal.Length+Petal.Width, data=training_nor, hidden=1)
model_net
plot(model_net)


```

![1569220885686](R_2.assets/1569220885686.png)

```r
#분류모델 정확도(성능) 평가
model_result <- compute(model_net, testing_nor[c(1:4)])
model_result$net.result
             [,1]
 [1,] -0.018958176
 [2,] -0.018692626
 [3,] -0.002037639
 [4,] -0.020735184
 [5,] -0.009701616
 [6,] -0.021807507
 [7,]  0.003149724
 [8,] -0.001888338
 [9,] -0.006976969
[10,] -0.019281390
[11,]  0.010225865
[12,] -0.022879081
[13,] -0.022000577
[14,]  0.573718570
[15,]  0.599948003
[16,]  0.563159308
[17,]  0.887054552
[18,]  0.720581624
[19,]  0.832972367
[20,]  0.493459578
[21,]  0.632243314
[22,]  0.564258669
[23,]  0.744402613
[24,]  0.470220837
[25,]  1.078712978
[26,]  0.975119795
[27,]  0.995049595
[28,]  1.031717877
[29,]  0.823619428
[30,]  0.972659780
[31,]  0.964095035
[32,]  1.098285069
[33,]  0.859984384
[34,]  0.955917370
[35,]  0.968422676
[36,]  1.067126677
[37,]  0.900784552
[38,]  0.822895738
[39,]  0.776941641
[40,]  0.713709080
[41,]  0.958526409
[42,]  1.019150753
[43,]  1.040300842
[44,]  0.933780256
[45,]  0.836360872

#상관관계분석 : 상관계수로 두 변수 간의 선형관계의 강도 측정
#예측된 꼭 종류와 실제 관측치 사이의 상관관계 측정
cor(model_result$net.result, testing_nor$Species2)
          [,1]
[1,] 0.9563346
#은닉층 2개
model_net2 <- neuralnet(Species2 ~ Sepal.Length+ Sepal.Width+Petal.Length+Petal.Width, 
               data=training_nor, hidden=2, algorithm="backprop", learningrate=0.01)
model_net2
plot(model_net2)

```

![1569220957722](R_2.assets/1569220957722.png)

```r


#분류모델 정확도(성능) 평가
model_result2 <- compute(model_net, testing_nor[c(1:4)])
model_result2$net.result
              [,1]
 [1,] -0.018958176
 [2,] -0.018692626
 [3,] -0.002037639
 [4,] -0.020735184
 [5,] -0.009701616
 [6,] -0.021807507
 [7,]  0.003149724
 [8,] -0.001888338
 [9,] -0.006976969
[10,] -0.019281390
[11,]  0.010225865
[12,] -0.022879081
[13,] -0.022000577
[14,]  0.573718570
[15,]  0.599948003
[16,]  0.563159308
[17,]  0.887054552
[18,]  0.720581624
[19,]  0.832972367
[20,]  0.493459578
[21,]  0.632243314
[22,]  0.564258669
[23,]  0.744402613
[24,]  0.470220837
[25,]  1.078712978
[26,]  0.975119795
[27,]  0.995049595
[28,]  1.031717877
[29,]  0.823619428
[30,]  0.972659780
[31,]  0.964095035
[32,]  1.098285069
[33,]  0.859984384
[34,]  0.955917370
[35,]  0.968422676
[36,]  1.067126677
[37,]  0.900784552
[38,]  0.822895738
[39,]  0.776941641
[40,]  0.713709080
[41,]  0.958526409
[42,]  1.019150753
[43,]  1.040300842
[44,]  0.933780256
[45,]  0.836360872
> cor(model_result2$net.result, testing_nor$Species2)
          [,1]
[1,] 0.9563346
```





## 군집분석 비지도 학습



### 1. 군집분석

#### 1.1 비지도 학습

- 데이터에 의한 학습을 통해 최적의 판단이나 예측을 가능하게 해주는 기계학습 방법의 하나로 어떤 입력에 대해서 어떤 결과가 출력되는지 사전지식이 없는 상태에서 컴퓨터 스스로 공통점과 차이점 등의 패턴을 찾아서 규칙(rule)을 생성하고, 분석 결과를 도출해내는 방식
- Y변수(정답)가 없기 때문에 검정 데이터를 이용하여 모델을 평가할 수 없다.
- =종속변수가 없다.

#### 1.2 군집분석

- 데이터 간의 유사도를 정의하고 그 유사도에 가까운 것부터 순서대로 합쳐 가는 방법으로 그룹(군집)을 형성한 후 각 그룹의 성격을 파악하거나 그룹 간의 비교분석을 통해서 데이터 전체의 구조에 대한 이해를 돕고자 하는 **탐색적 분석 방법**
- 유사도 거리(distance)를 이용 – 유클리디안(Euclidean) 거리도 측정한 거리정보를 이용해서 분석 대상을 몇 개의 집단으로 분류한다. 
- 군집분석에 의해서 그룹화된 군집은 변수의 특성이 그룹 내적으로는 동일하고, 외적으로는 이질적인 특성을 갖는다.
- 용도 – 고객의 충성도에 따라서 몇 개의 그룹으로 분류하고, 그룹별로 맞춤형 마케팅 및 프로모션 전략을 수립하는데 활용된다.

##### 1.2.1 목적

- 데이터 셋 전체를 대상으로 서로 유사한 개체 들을 몇 개의 군집으로 세분화하여 대상 집단을 정확하게 이해하고, 효율적으로 활용하기 위함

##### 1.2.2 중요사항

- 군집화를 위해서 거리 측정에 사용되는 변인은 비율척도나 등간척도이어야 하며, 인구 통계적 변인, 구매패턴 변인, 생활 패턴 변인 등이 이용된다.
- 군집분석에 사용되는 입력 자료는 변수의 측정단위와 관계없이 그 차이에 따라 일정하게 거리를 측정하기 때문에 변수를 표준화하여 사용하는 것이 필요하다.
- 군집화 방법에 따라 계층적 군집분석과 비계층적 군집분석으로 분류된다. 

##### 1.2.3 군집분석 이용 요인

- 인구 통계적 변인 : 거주지, 성별, 나이, 교육수준, 직업, 소득수준 등
- 구매패턴 변인 : 구매상품, 1회 평균 거래액, 구매횟수, 구매주기 등
- 생활패턴 변인 : 생활습관, 가치관, 성격, 취미 등

##### 1.2.4 특징

- 전체적인 데이터 구조를 파악하는데 이용된다.
- 관측대상 간 유사성을 기초로 비슷한 것끼리 그룹화(Clustering)한다.
- 유사성은 유클리디안 거리를 이용한다
- 분석결과에 대한 가설 검정이 없다
- 반응변수(y 변수)가 존재하지 않는 데이터마이닝 기법이다.
- 규칙(Rule)을 기반으로 계층적인 트리 구조를 생성한다.
- 활용분야는 구매패턴에 따른 고객분류, 충성도에 따른 고객분류 등

> 데이터 마이닝?
>
>  대규모 데이터에 포함된 유용한 정보를 발견하는 과정으로 데이터에 숨겨진 규칙과 패턴을 이용하여 광맥을 찾아내듯이 기존에 알려지지 않은 유용한 정보를 발견해 내는 기법

##### 1.2.5 절차

1. 분석 대상의 데이터에서 군집분석에 사용할 변수 추출
2. 계층적 군집분석을 이용한 대략적인 군집의 수 결정
3. 계층적 군집분석에 대한 타당성 검증(ANOVA 분석)
4.  비계층적 군집분석을 이용한 군집분류
5.  분류된 군집의 특성 파악 및 업무 적용

#### 1.3 유클리디안 거리

1. 두 점 사이의 거리를 계산하는 방법
2. 관측대상 p와 q의 대응하는 변량 값의 차가 작으면, 두 관측대상은 유사하다고 정의하는 식
3. 유클리디안 거리 계산식은 관측대상 p와 q의 대응하는 변량 값의 차의 제곱의 합에 제곱근을 적용한 결과이다.

![1569226363613](R_2.assets/1569226363613.png)

- matrix 객체를 대상으로 dist() 함수를 이용하여 유클리디안 거리를 생성한다.

  `dist( x, method=“euclidean”)`

- matrix 객체의 값이 서로 가까울수록 유클리디안 거리값이 작은 값으로 나타나고, 거리가 멀수록 큰 값으로 나타난다.

###### 1.3.1 실습

```r
> x<-matrix(1:9, nrow=3,by=T)
> x
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9
> dist<-dist(x,method="euclidean")
> dist
          1         2
2  5.196152          
3 10.392305  5.196152
# 1과 2, 2와 3은 유클리디안 거리가 1과 3보다 가깝다. 1과 3이 가장 멀다. 

#### 직접 계산해보자
#1행과 2행의 변량의 차의 제곱의 합
> s<-sum((x[1,]-x[2,])^2) # 또는 ** 하면 제곱이 된다.
> sqrt(s)
[1] 5.196152

> s<-sum((x[1,]-x[3,])^2) # 또는 ** 하면 제곱이 된다.
> sqrt(s)
[1] 10.3923

```

###  2 . 계층적 군집분석 절차

1. 개별대상 간의 거리에 의하여 가장 가까운 대상부터 결합하여 나무 모양의 계층구조를 상향식(Bottom-up)으로 만들어가면서 군집을 형성하는 방법
2. 계층적 군집 분석은 군집이 형성되는 과정을 파악할 수 있다는 장점과 자료의 크기가 큰 경우 분석이 어렵다는 단점이 있다.
3. 군집화된 결과를 **plot() **함수를 이용하여 시각화하면 덴드로그램(Dendrogram)에 의해서 클러스터 형태로 시각화해 준다
4. 덴드로그램에서 Height는 해당 군집에 대한 유클리디안 거리를 의미한다.
5. 계층적 군집분석결과에서 분석자가 원하는 군집수 만큼 인위적으로 군집을 만들 수 있다 (stats::cutree(계층적 군집분석결과, k=군집수))

##### 2.1 계층적 군집분석 분류

- 단일결합기준(최소거리 이용)
- 완전결합기준(최대거리 이용)
- 평균결합기준(평균거리 이용)
- 중심결합 기준(중심 값의 거리 이용)
- ward(유클리디안 제곱거리)  

###### 2.1.1 실습

```r
######## 유클리디안 거리 계산 계층적 군집 분석##################
x<-matrix(1:9,nrow=3,by=T)
x
dist<-dist(x, method="euclidean")
dist

###유클리드 거리 matrix를 이용한 군집화
hc<-hclust(dist)
plot(hc)


```

###### 2.1.2 실습 2

```r
> interview<-read.csv("./4/interview.csv")
> names(interview)
[1] "no"       "가치관"   "전문지식" "발표력"   "인성"     "창의력"  
[7] "자격증"   "종합점수" "합격여부"

> head(interview)
   no 가치관 전문지식 발표력 인성 창의력 자격증 종합점수 합격여부
1 101     20       15     15   15     12      1       77     합격
2 102     19       15     14   18     13      1       79     합격
3 103     12       16     20   11      7      1       66   불합격
4 104     18       15     15   14     13      1       75     합격
5 105      9       18     20    9      5      0       61   불합격
6 106     20       13     18   15     11      1       77     합격

> #유클리디안 거리 계산
> interview_df<-interview[c(2:7)]
> idist<-dist(interview_df)
> head(idist)
[1]  3.464102 11.445523  2.449490 15.524175  3.741657 14.142136
hc<-hclust(idist)
plot(hc,hang=-1) # hang=-1 은 덴드로그램에서 음수값을 제거
rect.hclust(hc,k=3,border="red")

```

![1569227778039](R_2.assets/1569227778039.png)

```r
#세가지의 군집 분석이 된 것이다. 
#(8,10,7,12,15),(2,1,4,6,13),(5,14,9,3,11) 이렇게 3그룹
# 군집별 특성을 보기 위해 군집별 subset생성
> g1<-subset(interview,no==108|no==117|no==112|no==115|no==110)
> g2<-subset(interview,no==102|no==101|no==106|no==106|no==113)
> g3<-subset(interview,no==105|no==114|no==109|no==103|no==111)
> summary(g1)
       no            가치관         전문지식         발표력     
 Min.   :108.0   Min.   :13.00   Min.   :17.00   Min.   :10.00  
 1st Qu.:109.5   1st Qu.:13.75   1st Qu.:17.75   1st Qu.:10.75  
 Median :111.0   Median :14.50   Median :18.50   Median :11.50  
 Mean   :111.2   Mean   :14.50   Mean   :18.50   Mean   :11.50  
 3rd Qu.:112.8   3rd Qu.:15.25   3rd Qu.:19.25   3rd Qu.:12.25  
 Max.   :115.0   Max.   :16.00   Max.   :20.00   Max.   :13.00  
      인성           창의력          자격증     종합점수       합격여부
 Min.   : 8.00   Min.   :16.00   Min.   :0   Min.   :65.00   불합격:4  
 1st Qu.: 8.75   1st Qu.:16.75   1st Qu.:0   1st Qu.:68.75   합격  :0  
 Median : 9.50   Median :17.50   Median :0   Median :72.50             
 Mean   : 9.25   Mean   :17.75   Mean   :0   Mean   :71.50             
 3rd Qu.:10.00   3rd Qu.:18.50   3rd Qu.:0   3rd Qu.:75.25             
 Max.   :10.00   Max.   :20.00   Max.   :0   Max.   :76.00             
> summary(g2)
       no            가치관         전문지식         발표력     
 Min.   :101.0   Min.   :18.00   Min.   :13.00   Min.   :14.00  
 1st Qu.:101.8   1st Qu.:18.75   1st Qu.:13.75   1st Qu.:14.75  
 Median :104.0   Median :19.50   Median :14.50   Median :15.50  
 Mean   :105.5   Mean   :19.25   Mean   :14.25   Mean   :15.75  
 3rd Qu.:107.8   3rd Qu.:20.00   3rd Qu.:15.00   3rd Qu.:16.50  
 Max.   :113.0   Max.   :20.00   Max.   :15.00   Max.   :18.00  
      인성           창의력          자격증     종합점수       합격여부
 Min.   :12.00   Min.   :10.00   Min.   :1   Min.   :70.00   불합격:0  
 1st Qu.:14.25   1st Qu.:10.75   1st Qu.:1   1st Qu.:75.25   합격  :4  
 Median :15.00   Median :11.50   Median :1   Median :77.00             
 Mean   :15.00   Mean   :11.50   Mean   :1   Mean   :75.75             
 3rd Qu.:15.75   3rd Qu.:12.25   3rd Qu.:1   3rd Qu.:77.50             
 Max.   :18.00   Max.   :13.00   Max.   :1   Max.   :79.00             
> summary(g3)
       no            가치관      전문지식        발표력          인성   
 Min.   :103.0   Min.   : 9   Min.   :13.0   Min.   :18.0   Min.   : 9  
 1st Qu.:105.0   1st Qu.:10   1st Qu.:14.0   1st Qu.:19.0   1st Qu.:10  
 Median :109.0   Median :11   Median :15.0   Median :20.0   Median :11  
 Mean   :108.4   Mean   :11   Mean   :15.2   Mean   :19.4   Mean   :11  
 3rd Qu.:111.0   3rd Qu.:12   3rd Qu.:16.0   3rd Qu.:20.0   3rd Qu.:12  
 Max.   :114.0   Max.   :13   Max.   :18.0   Max.   :20.0   Max.   :13  
     창의력        자격증       종합점수      합격여부
 Min.   :5.0   Min.   :0.0   Min.   :57.0   불합격:5  
 1st Qu.:5.0   1st Qu.:0.0   1st Qu.:61.0   합격  :0  
 Median :6.0   Median :0.0   Median :64.0             
 Mean   :6.2   Mean   :0.4   Mean   :62.8             
 3rd Qu.:7.0   3rd Qu.:1.0   3rd Qu.:66.0             
 Max.   :8.0   Max.   :1.0   Max.   :66.0 


#summary(g1)  #종합점수 평균:71.6, 인성평균 :9.4 , 자격증 없음
#summary(g2)  #종합점수 평균:75.6, 인성평균 :15 , 자격증 있음
#summary(g3)  #종합점수 평균:62.8, 인성평균 :11 , 자격증 있음, 없음

```



### 3.  비계층적 군집분석

- 군집의 수가 정해진 상태에서 군집의 중심에서 가장 가까운 개체를 하나씩 포함해 나가는 방법
- 군집수를 미리 알고 있는 경우 군집 대상의 분포에 따라 군집의 초기값을 설정해 주면, 초기값에 가장 가까운 거리에 있는 대상을 하나씩 더해 가는 방식으로 군집화를 수행
- 계층적 군집분석을 통해 대략적인 군집의 수를 파악하고 이를 초기 군집 수로 설정하여 비계층적 군집분석을 수행하는 것이 효과적
  K-means clustering  (stats::kmeans())

#### 3.1 장단점



### 연습문제

###### 분류분석 연습문제1

```r
  # ggplot2::mpg 데이터 셋
  # model(모델), displ(엔진 크기) , cyl(실린더 수), drv(구동 방식)
  # 종속변수 : 고속도로 주행거리(hwy)

  library(ggplot2)
  str(mpg)
  data.frame:	234 obs. of  11 variables:
 $ manufacturer: chr  "audi" "audi" "audi" "audi" ...
 $ model       : chr  "a4" "a4" "a4" "a4" ...
 $ displ       : num  1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...
 $ year        : int  1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...
 $ cyl         : int  4 4 4 4 6 6 6 4 4 4 ...
 $ trans       : chr  "auto(l5)" "manual(m5)" "manual(m6)" "auto(av)" ...
 $ drv         : chr  "f" "f" "f" "f" ...
 $ cty         : int  18 21 20 21 16 18 18 18 16 20 ...
 $ hwy         : int  29 29 31 30 26 26 27 26 25 28 ...
 $ fl          : chr  "p" "p" "p" "p" ...
 $ class       : chr  "compact" "compact" "compact" "compact" ...
  
t<-sample(1:nrow(mpg),120) # x 범위에서 y만큼 sample 데이터를 생성하는 함수
train<-mpg[t,] #학습데이터
test<-mpg[-t,]# 검정데이터
test$drv<-factor(test$drv)# 구동방식식범주형 변환
# 이걸해야만 chr 형태의 글자또한 범주형으로 인식하여 결과에 넣을수 있다. 안하면 character라서 안된다는 오류가 뜬다.
  
formula<-hwy ~ displ+cyl+drv
hwy_ctree<-ctree(formula,data=test) 
plot(hwy_ctree)

```

![1569203580989](R_2.assets/1569203580989.png)

###### 분류분석 연습문제2

```r
#####################분류분석 연습문제 2  ####################
install.packages("arules")
library(arules)
data("AdultUCI")
#성인 대상 인구 소득에 관한 설문 조사 데이터
#48,842 관측치와 15개변수
age, workclass(직업 :4개), education(교육수준: 16개), marital-status(결혼상태: 6개), occupation(직업:12개), relationship(관계: 6개), race(인종:아시아계, 백인), sex(성별), capital-gain(자본이득), capital-loss(자본손실), fnlwgt(미지의 변수), hours-per-week(주당 근무시간), native-country(국가), income(소득)

#10,000개 관측치를 샘플링해서
자본이득에 영향을 미치는 변수를 분석하기 위해 
capital-gain, hours-per-week, education-num, race, age, income 변수로만 구성된 데이터프레임을 생성한후 분류모델 생성하고 예측하시오


install.packages("arules")
  library(arules)
  data("AdultUCI")
  AdultUCI
  str(AdultUCI)

  sam<-sample(1:nrow(AdultUCI),10000)  

  train<-AdultUCI[sam,]  
  test<-AdultUCI[-sam,]  
  names(AdultUCI)
 
  names(test)<-c('age','workclass','fnlwgt','education','educationnum','maritalstatus', 'occupation',
                    'relationship','race','sex','capitalgain','capitalloss','hoursperweek','nativecountry','income')
  
  formula<-capitalgain ~ hoursperweek+educationnum+race+age+income
  capital_ctree<-ctree(formula,data=test)  
plot(capital_ctree)


분석결과 :가장 영향력 있는 것은 수입! 두번째는 education=num이다.
```

![1569206292628](R_2.assets/1569206292628.png)

###### 분류분석 연습문제 3 

```r
weather<-read.csv("./4/weather.csv")
#RainTomorrow 컬럼을 종속변수로
#날씨 요인과 관련없는 Date와 RainToday컬럼을 제외한 나머지 변수를 x변수로
#지정하여 분류 모델 생성성
str(weather)

fomula<-RainTomorrow~ .

test_c<-ctree(fomula,data=weather)
plot(test_c)


```

![1569211831819](R_2.assets/1569211831819.png)

###### 분석분류 연습문제 4

```r
########################분류분석 연습문제 4#######################
# mpg 데이터 셋을 대상으로 7:3 비율로 학습데이터와 검정데이터로 각각 샘플링한 후
각 단계별로 분류분석을 수행하시오.
조건) 변수모델링 : x변수(displ + cyl + year), y변수(cty)

단계1 : 학습데이터와 검증데이터 샘플링
단계2 : formula 생성
단계3 : 학습데이터에 분류모델 적용
단계4 : 검정데이터에 분류모델 적용 
단계5 : 분류분석 결과 시각화 
단계6 : 분류분석 결과 해설 



```

###### 분석분류 연습문제 5

```r
########################분류분석 연습문제 5##################
04. weather 데이터를 이용하여 다음과 같은 단계별로 분류분석을 수행하시오.
조건1) rpart() 함수 이용 분류모델 생성
조건2) 변수 모델링 :
y변수(RainTomorrow), x변수(Date와 RainToday 변수 제외한 나머지 변수)
조건3) 비가 올 확률이 50% 이상이면 ‘Yes Rain’, 50% 미만이면 ‘No Rain’으로 범주화

단계1 : 데이터 가져오기
library(rpart)
weather = read.csv("./data/weather.csv", header=TRUE)
단계2 : 데이터 샘플링 
단계3 : 분류모델 생성 
단계4 : 예측치 생성 : 검정데이터 이용 
단계5 : 예측 확률 범주화('Yes Rain', 'No Rain')
단계6 : 혼돈 행렬(confusion matrix) 생성 및 분류정확도 구하기 
```





###### 분류분석 연습문제 답

```r
#####################분류분석 연습문제 1  ####################
# ggplot2::mpg 데이터 셋
# model(모델), displ(엔진 크기) , cyl(실린더 수), drv(구동 방식)
# 종속변수 : 고속도로 주행거리(hwy)
library(ggplot2)
data(mpg)
t <-sample(1:nrow(mpg), 120)
train <- mpg[t, ]
test <- mpg[-t, ]
test$drv <- factor(test$drv)  #구동방식 범주형 변환
formula <- hwy ~ disp+cyl+drv
hwy_ctree <- ctree(formula, data=test)
plot(hwy_ctree)

분석 결과 : 엔진 크기가 작으면서 전륜구동(f)이나 후륜(r) 구동 방식인 경우 고속도로 주행거리가 가장 좋고, 
엔진 크기가 크고, 사륜구동 방식이면 실린더 수가 많은 경우 고속도로 주행거리가 적은 것으로 분석된다.

#####################분류분석 연습문제 2  ####################
install.packages("arules")
library(arules)
data("AdultUCI")
#성인 대상 인구 소득에 관한 설문 조사 데이터
#48,842 관측치와 15개변수
age, workclass(직업 :4개), education(교육수준: 16개), marital-status(결혼상태: 6개), occupation(직업:12개), relationship(관계: 6개), race(인종:아시아계, 백인), sex(성별), capital-gain(자본이득), capital-loss(자본손실), fnlwgt(미지의 변수), hours-per-week(주당 근무시간), native-country(국가), income(소득)

#10,000개 관측치를 샘플링해서
자본이득에 영향을 미치는 변수를 분석하기 위해 
capital-gain, hours-per-week, education-num, race, age, income 변수로만 구성된 데이터프레임을 생성한후 분류모델 생성하고 예측하시오
names(AdultUCI)
set.seed(1234)
choice <- sample(1:nrow(AdultUCI), 10000)
choice

adult.df <- AdultUCI[choice, ]
str(adult.df)

capital <- adult.df$'capital-gain'
hours <- adult.df$'hours-per-week'
education <- adult.df$'education-num'
race <- adult.df$race
age <- adult.df$age
income <- adult.df$income

adult_df <- data.frame(capital=capital, age=age , hours=hours,
    education=education, income=income)
str(adult_df)

formula <- capital ~ income+education+hours+age

adult_ctree <- ctree(formula, data=adult_df)

plot(adult_ctree)

#분석결과 : 자본이득(capital)에 가장 큰 영향을 미치는 변수는 income이고, 두번째는 education 변수이다.
수입이 많고 교육수준이 높을수록 자본이득이 많은 것으로 분석된다.

#분류 모델의 조건에 맞는 subset 생성
adultResult <- subset(adult_df, adult_df$income=='large' &  adult_df$education > 14)
length(adultResult$education)
summary(adultResult$capital) 
boxplot(adultResult$capital)

#income이 large이고 education이 14를 초과한 경우, 
자본이득 평균은 7,170
############분류분석 연습문제 3 ########################
weather <- read.csv("./data/weather.csv", header=TRUE)

#RainTomorrow 컬럼을 종속변수로 
# 날씨 요인과 관련없는 Date와 RainToday컬럼을 제외한 나머지 변수를 x변수로 지정하여 분류 모델 생성하고 모델을 평가하시오

str(weather)
names(weather)
weather.df <- rpart(RainTomorrow ~ ., data=weather[, c(-1, -14)], cp=0.01) 
X11()
plot(weather.df)
text(weather.df, use.n=T, cex=0.7)

#분석 결과 : 분기조건이 True이면 왼쪽으로 분류되고, False
이면 오른쪽으로 분류된다.
#rpart()함수의 cp속성값을 높이면 가지 수가 적어지고, 낮추면 가지 수가 많아진다. cp 기본값은 0.01

weather_pred <- predict(weather.df , weather)
weather_pred

#y의 범주로 코딩 변환 : Yes(0.5이상), No(0.5미만)
#rpart의 분류모델 예측치는 비 유무를 0~1사이의 확률값으로 예측하다 
# 혼돈매트릭스를 이용하여 분류정확도를 구하기 위해 범주화 코딩 변경
weather_pred2 <- ifelse(weather_pred[,2] >= 0.5, 'Yes', 'No')
table(weather_pred2, weather$RainTomorrow)
weather_pred2  No Yes
          No  278  13
          Yes  22  53
> (53+278)/nrow(weather) #(Yes,Yes+No,No /nrow())
[1] 0.9043716

########################분류분석 연습문제 4#######################
# mpg 데이터 셋을 대상으로 7:3 비율로 학습데이터와 검정데이터로 각각 샘플링한 후
각 단계별로 분류분석을 수행하시오.
조건) 변수모델링 : x변수(displ + cyl + year), y변수(cty)

단계1 : 학습데이터와 검정 데이터 생성
idx <- sample(1: nrow(mpg), nrow(mpg) * 0.7)
train <- mpg[idx, ] # 학습데이터
dim(train)
test <- mpg[-idx, ] # 검정데이터
dim(test)

단계2 : formula 생성
# 도시 주행마일수 <- 실린더, 엔진크기, 제조년도 
formula <- cty ~ displ + cyl + year

단계3 : 학습데이터에 분류모델 적용
mpg_train <- ctree(formula, data=train)

단계4 : 검정데이터에 분류모델 적용
mpg_test <- ctree(formula, data=test)

단계5 : 분류분석 결과 시각화
plot(mpg_test)

단계6 : 분류분석 결과 해설
실린더가 5이하이면 엔진크기에 의해서 23개가 분류되고, 실린더가 5이상이고,
6이하이면 27개가 분류되고, 6을 초과한 경우 21개가 분류된다.

########################분류분석 연습문제 5##################
weather 데이터를 이용하여 다음과 같은 단계별로 분류분석을 수행하시오.
조건1) rpart() 함수 이용 분류모델 생성
조건2) 변수 모델링 :
y변수(RainTomorrow), x변수(Date와 RainToday 변수 제외한 나머지 변수)
조건3) 비가 올 확률이 50% 이상이면 ‘Yes Rain’, 50% 미만이면 ‘No Rain’으로 범주화

단계1 : 데이터 가져오기
library(rpart)
weather = read.csv("./data/weather.csv", header=TRUE)
 
단계2 : 데이터 샘플링
weather.df <- weather[, c(-1,-14)]
nrow(weather.df)
idx <- sample(1:nrow(weather.df), nrow(weather.df)*0.7)
weather_train <- weather.df[idx, ]
weather_test <- weather.df[-idx, ]

단계3 : 분류모델 생성
weather_model <- rpart(RainTomorrow ~ ., data = weather.df)
weather_model # Humidity 중요변수

단계4 : 예측치 생성 : 검정데이터 이용
weater_pred <- predict(weather_model, weather_test)
weater_pred

단계5 : 예측 확률 범주화('Yes Rain', 'No Rain')
weater_class <- ifelse(weater_pred[,1] >=0.5, 'No Rain', 'Rain')

단계6 : 혼돈 행렬(confusion matrix) 생성 및 분류정확도 구하기
table(weater_class, weather_test$RainTomorrow)
# weater_class No Yes
# No Rain 83 6
# Rain 2 19
# (83 + 19) / nrow(weather_test)
# [1] 0.9272727
 
```

